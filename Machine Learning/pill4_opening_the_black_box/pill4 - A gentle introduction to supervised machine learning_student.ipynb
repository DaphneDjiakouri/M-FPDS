{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pill 5 - A gentle introduction to supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style = \"border-radius:10px\">**DEPENDENCIES:** `ipywidgets` dependency.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Disectioning machine learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to learn, any algorithm has to define at least three components:\n",
    "\n",
    "+ **The model class/hypothesis space** defines the family of mathematical models that will be used. The target decision boundary will be approximated from one element of this space. For example, we can consider the class of linear models. In this case our decision boundary will be a line if the problem is defined in ${\\bf R}^2$ and the model class is the space of all posible lines in ${\\bf R}^2$. \n",
    "\n",
    "    Model classes define the geometric properties of the decision function. There are different taxonomies but the most well-known are the *families* of **linear** and **non-linear** models. These families usually depend on some parameters. And the solution to a learning problem is the selection of a particular set of parameters, i.e. the selection of an instance model from the model class space. The model class space is also called **hypothesis space**.\n",
    "\n",
    "    The selection of the best model will depend on our problem and what we want to obtain from the problem. The primary goal in learning is usually achieving the minimum error/maximum performance. But according to what else we want from the algorithm we will find different algorithms. Other common desirable properties are interpretability, behavior in front of missing data, fast training, etc.\n",
    "\n",
    "\n",
    "+ **The problem model** formalizes and encodes the desired properties of the solution. In many cases this formalization takes the form of an optimization problem. In it most basic instantiation, the problem model can be the **minimization of an error function**. The error function measures the difference between our model and the target one. Informally speaking, in a classification problem it measures how \"irritated\" we are when our model misses the right label of a training sample. For example, in classification the ideal error function is the **0-1 loss**. This function takes value $1$ when we incorrectly classify a training sample and zero otherwise. In this case, it can be intrepreted that one is only irritated by \"one unit of irritation\" when one sample is misclassified.\n",
    "\n",
    "    Problem model can also be used to impose other constraints on our solution, such as finding a smooth approximation, small complexity model, sparse solution, etc.\n",
    "    \n",
    "    \n",
    "+ **The learning algorithm** is an optimization/search method or algorithm that given a model class fits it to the training data according to the error function. According to the nature of our problem there are many different algorithms. In general, we are talking about finding the minimum error approximation or maximum probable model. In those cases, if the problem is convex/quasi-convex we will typically use first or second order methods (i.e. gradient descent, coordinate descent, Newton's method, Interior Point methods, etc). Other searching techniques such as genetic algorithms or monte-carlo techniques can be used if we do not have access to the derivatives of the objective function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#from sklearn import \n",
    "import numpy as np\n",
    "#Create some data\n",
    "X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n",
    "y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n",
    "\n",
    "#Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[0:40,0],X[0:40,1],color='r')\n",
    "plt.scatter(X[40:,0],X[40:,1],color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import \n",
    "import numpy as np\n",
    "#Create some data\n",
    "X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n",
    "y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n",
    "\n",
    "#Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "def human_learning_algorithm(X,y):\n",
    "    \n",
    "    plt.scatter(X[0:40,0],X[0:40,1],color='r')\n",
    "    plt.scatter(X[40:,0],X[40:,1],color='b')\n",
    "    #plt.scatter(X[0:40,0],X[0:40,1],color='r')\n",
    "    #plt.scatter(X[40:,0],X[40:,1],color='b')    \n",
    "    delta = 0.025\n",
    "    xx = np.arange(-5.0, 10.0, delta)\n",
    "    yy = np.arange(-5.0, 10.0, delta)\n",
    "    XX, YY = np.meshgrid(xx, yy)\n",
    "    Xf = XX.flatten()\n",
    "    Yf = YY.flatten()\n",
    "    sz=XX.shape\n",
    "    data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "\n",
    "    def hml_display(w0,w1,offset):\n",
    "        w=np.array([w0,w1])\n",
    "        w.shape=(2,1)\n",
    "        #Evaluate the model for a given weight\n",
    "        Z = data.dot(w)+offset\n",
    "        Z.shape=sz\n",
    "        plt.scatter(X[0:40,0],X[0:40,1],color='r')\n",
    "        plt.scatter(X[40:,0],X[40:,1],color='b')\n",
    "        plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,10,-5,10),alpha=0.3, vmin=-15, vmax=15)\n",
    "        plt.contour(XX,YY,Z,[0])\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(9,9)\n",
    "   \n",
    "    #Ipython 2.0      \n",
    "    interact(hml_display, w0=(-10.,10.), w1=(-10.,10.), offset=(-20.,40.));\n",
    "    #Ipython 1.1 back compatibility\n",
    "    #w0=-10. #-10.,10.\n",
    "    #w1=-10. #-10.,10.\n",
    "    #offset=-20. #-20.,40.\n",
    "    #hml_display(w0,w1,offset) \n",
    "\n",
    "    \n",
    "human_learning_algorithm(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**QUESTION:** Describe the process you used for fitting the classifier\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style=\"border-radius:10px\">**Some notes on the learning process**\n",
    "<p>\n",
    "The main goal of any learning process is to achive the maximum predictive power (*accuracy*). This is minimize the error. However, there are three other important properties we usually desire our models to have:\n",
    "\n",
    "<p>\n",
    "<li> **Simplicity** - how much fiddling do we need for the method to work? Can I modify it to handle the particularities of my problem?\n",
    "<li> **Speed** - How long does it take to train a reliable model? (training time) Can I use it in embedded and real time applications? (testing time), How long do I have to wait for processing my 1YB (yottabyte - 1e24 Bytes) dataset?\n",
    "<li> **Interpretability** - Why did it make this predictions?\n",
    "\n",
    "<p>\n",
    "It happens that accuracy trades off with all the rest of the desirable properties. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we can represent the problem of supervised machine learning as in the following scheme:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./files/learning_problem-eps-converted-to.jpg\" width = 400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. First steps into a model: Linear regression\n",
    "\n",
    "Consider a two dimensional problem such as the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "x = 4+np.random.normal(size=(20,1))\n",
    "y = 0.5 + 1.2*x + 0.5*np.random.normal(size=(20,1))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every problem we have to decide the **model class**, the **cost function**, and the **learning/searching algorithm**.\n",
    "\n",
    "In this problem we could decide to use as **model class** the set of *linear models*, i.e. $$\\mathcal{H}(w_0,w_1) = \\hat{y} = w_1 x + w_0,$$\n",
    "where $w_1$ and $w_0$ are the parameters of the model. \n",
    "\n",
    "Our **cost function** could be the *mean least squares* function, i.e. $$\\mathcal{L}(y_i,\\hat{y}_i) = \\frac{1}{N}\\sum_{i=0}^N (y_i - \\hat{y}_i)^2$$, where $\\hat{y}$ is the predicted value.\n",
    "\n",
    "Using these two elements we can model the problem of learning as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{w_0,w_1}{\\text{minimize}}&& \\frac{1}{N}\\sum_{i=0}^N (y_i - (w_1x_i + w_0))^2.\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last component is the way of solving this problem. Fortunately for us we know about *optimization* and *numerical linear algebra*. Thus we can proceed in different directions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with using a very simple and intuitive idea:\n",
    "\n",
    "<ol>\n",
    "<li>Start with a random point, ${\\bf w}^0$.</li>\n",
    "<li>Change the parameters in such a way that we reduce the cost function</li>\n",
    "$$\\mathcal{L}({\\bf w}^{t})\\leq \\mathcal{L}({\\bf w}^{t-1})$$\n",
    "<li>Repeat until we can not reduce the cost function</li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us refine the process\n",
    "<ol>\n",
    "<li>Start with a random point, ${\\bf w}^0$.</li>\n",
    "<li>Select a descent direction $\\Delta {\\bf w}$ and a step size $\\eta$ (this last can be selected by line search).</li>\n",
    "<li>Update the parameters:</li>\n",
    "$$ {\\bf w}^{t} = {\\bf w}^{t-1} + \\eta \\Delta {\\bf w}$$\n",
    "\n",
    "<li>Repeat 2-3 until the stoping criterion is met (This can be a certain number of iterations or a desired error tolerance).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most well known technique in this kind of techniques is **Steepest descent** or **Gradient descent**, where $\\Delta {\\bf w} = -\\nabla_{\\bf w} \\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve the problem we have to compute the gradient of the loss function. We can easily derive the expression considering matrix calculus. Let us express the linear model as the inner product of the parameters and the data point. For the sake of simplicity consider an extended data example as \n",
    "\n",
    "$$\\tilde{\\bf x} = \\left(\\begin{matrix} 1\\\\ {\\bf x} \\end{matrix}\\right)$$\n",
    "\n",
    "\n",
    "thus\n",
    "\n",
    "$$\\hat{y} = \\sum_{i=1}^d w_i x^{(i)} + w_0 = \\sum_{i=0}^d w_i \\tilde{x}^{(i)} = \\tilde{\\bf x}^T {\\bf w}.$$\n",
    "\n",
    "Moreover, given a data set with $N$ data points, where each one is stored as a column vector, we can define the design matrix as \n",
    "\n",
    "$${\\bf X} = (x_1, \\dots x_N).$$\n",
    "\n",
    "then, the output of our model for the whole data set is a prediction vector defined as\n",
    "\n",
    "$$\\hat{\\bf y} = \\tilde{\\bf X}^T {\\bf w}.$$\n",
    "\n",
    "The loss function can also be expressed as a matrix product,\n",
    "\n",
    "$$\\mathcal{L}(y_i,\\hat{y}_i) = \\frac{1}{N}\\sum_{i=0}^N (y_i - \\hat{y}_i)^2 = \\frac{1}{N} ({\\bf y}-\\hat{\\bf y})^T({\\bf y}-\\hat{\\bf y})$$\n",
    "\n",
    "Thus our problem can be rewriten as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{{\\bf w}}{\\text{minimize}}&& \\frac{1}{N}({\\bf y}-\\tilde{\\bf X}^T {\\bf w})^T({\\bf y}-\\tilde{\\bf X}^T {\\bf w}).\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can differentiate this expression with respect the vector ${\\bf w}$, considering the following rule \n",
    "\n",
    "$$\\partial (A B) = \\partial(A) B= + (A \\partial (B))^T.$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\\nabla_{\\bf w} \\mathcal{L} = - \\frac{2}{N} \\tilde{\\bf X}({\\bf y}-\\tilde{\\bf X}^T {\\bf w}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style= \"border-radius:10px\"> **EXERCISE/QUIZ:** Code a basic gradient descent for the problem. What is the value optained for $\\eta=0.01$ and 50 iterations if we initialize the weights to $(0,0)$? (Write down your solution, it will be asked for in the quiz later).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "\n",
    "n_iter = 50\n",
    "learning_rate = 0.01\n",
    "N = x.shape[0]\n",
    "x_augmented = np.c_[np.ones((x.shape[0],1)),x].T\n",
    "w = np.zeros((2,1))\n",
    "objective=np.zeros((n_iter,1))\n",
    "\n",
    "for i in range(n_iter):\n",
    "    #COMPLETE GRADIENT DESCENT HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to compute the value for the quiz\n",
    "\n",
    "assert w.shape==(2,1), 'The shape of the answer must be (2,1)'\n",
    "\n",
    "w_opt = np.array([[0.5],[1.2]])\n",
    "d = np.sqrt(np.dot((w_opt-w).T,(w_opt-w)))\n",
    "print('The distance between w* and the obtained w is: '+ str(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style= \"border-radius:10px\"> **EXERCISE/QUIZ:** Let us speed up the algorithm using a larger step. Solve the problem using the same code and $\\eta=0.1$  and 100 iterations (starting point still the zero vector). What is the resulting value of the optimal parameters?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style= \"border-radius:10px\"> **EXERCISE/QUIZ:** Repeat the former exercise but compute the value of the objective function at each iteration and plot it. This is called \"convergence plot\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed form solution for  least squares linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately for us, this problem has a close form solution. As you may recall from optimization the condition for a point being an extremum is \n",
    "\n",
    "$$\\nabla_{\\bf w} \\mathcal{L} = 0$$\n",
    "\n",
    "In this problem, we can write\n",
    "\n",
    "$$ - \\frac{2}{N} \\tilde{\\bf X}({\\bf y}-\\tilde{\\bf X}^T {\\bf w}) = 0 $$\n",
    "\n",
    "which, with simple manipulation gives us the following answer:\n",
    "\n",
    "$${\\bf w} = (\\tilde{\\bf X}\\tilde{\\bf X}^T)^{-1}\\tilde{\\bf X}{\\bf y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style= \"border-radius:10px\"> **EXERCISE/QUIZ:** Repeat the former exercise using the analytic solution and write down the value for the quiz.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_augmented = np.c_[np.ones((x.shape[0],1)),x].T\n",
    "\n",
    "#USE STANDARD LINEAR ALGEBRA HERE (np.linalg.pinv for the pseudo inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to compute the value for the quiz\n",
    "\n",
    "assert w.shape==(2,1), 'The shape of the answer must be (2,1)'\n",
    "\n",
    "w_opt = np.array([[0.5],[1.2]])\n",
    "d = np.sqrt(np.dot((w_opt-w).T,(w_opt-w)))\n",
    "print('The distance between w* and the obtained w is: '+ str(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A comparison between both solvers\n",
    "\n",
    "<table>\n",
    "<tr><td>Iterative methods</td><td>Analytic solution</td></tr>\n",
    "<tr><td>\n",
    "<ul>\n",
    "<li>Need to choose the learning rate $\\eta$. </li>\n",
    "<li>May be slow.</li>\n",
    "<li>Each iteration is usually cheap.</li>\n",
    "<li>Can be used with large data sizes.</li>\n",
    "</ul>\n",
    "</td><td>\n",
    "<ul>\n",
    "<li>Looks nicer</li>\n",
    "<li>\"No iterations are required\"</li>\n",
    "<li>Large computational cost. In the simplest case where we need matrix inversion, a naive method would need $O(n^3)$</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "state": {
    "8e3a213070f24224931d4edaac15e0af": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
