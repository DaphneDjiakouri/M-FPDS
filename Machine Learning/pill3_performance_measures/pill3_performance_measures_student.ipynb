{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pill 3: About performance\n",
    "\n",
    "**Outline**\n",
    "\n",
    "+ Accuracy metrics\n",
    "    + Error and accuracy\n",
    "    + Receiver operating curve\n",
    "    + Area under the curve\n",
    "    \n",
    "+ Model selection II. Crossvalidation.\n",
    "\n",
    "+ The unbalanced problem.\n",
    "\n",
    "+ Confusion matrix and partial performance measurements.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. More about the 'Churn' problem and accuracy metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling churn means to understand what keeps the customer engaged to our product. Its analysis goal is to predict or describe the **churn rate** i.e. the rate at which customer leave or cease the subscription to a service. Its value lies in the fact that engaging new customers is often more costly than retaining existing ones. For that reason subscription business-based companies usually have proactive policies towards customer retention.\n",
    "\n",
    "In this case study, we aim at building a machine learning based model for customer churn prediction on data from a Telecom company. Each row on the dataset represents a subscribing telephone customer. Each column contains customer attributes such as phone number, call minutes used during different times of day, charges incurred for services, lifetime account duration, and whether or not the customer is still a customer.\n",
    "\n",
    "This case is partially inspired in Eric Chiang's analysis of churn rate. Data is available from the University of California Irvine machine learning repositories data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The complete set of attributes is the following:\n",
    "\n",
    "+ State: categorical, for the 50 states and the District of Columbia\n",
    "+ Account length: integer-valued, how long an account has been active \n",
    "+ Area code: categorical\n",
    "+ Phone number: customer ID\n",
    "+ International Plan: binary feature, yes or no\n",
    "+ VoiceMail Plan: binary feature, yes or no\n",
    "+ Number of voice mail messages: integer-valued\n",
    "+ Total day minutes: continuous, minutes customer used service during the day\n",
    "+ Total day calls: integer-valued\n",
    "+ Total day charge: continuous\n",
    "+ Total evening minutes: continuous, minutes customer used service during the evening\n",
    "+ Total evening calls: integer-valued\n",
    "+ Total evening charge: continuous\n",
    "+ Total night minutes: continuous, minutes customer used service during the night\n",
    "+ Total night calls: integer-valued\n",
    "+ Total night charge: continuous\n",
    "+ Total international minutes: continuous, minutes customer used service to make international calls\n",
    "+ Total international calls: integer-valued\n",
    "+ Total international charge: continuous\n",
    "+ Number of calls to customer service: integer-valued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "churn_df = pd.read_csv('./files/churn.csv')\n",
    "col_names = churn_df.columns.tolist()\n",
    "\n",
    "print (\"Column names:\")\n",
    "print (col_names)\n",
    "\n",
    "to_show = col_names[:6] + col_names[-6:]\n",
    "\n",
    "print (\"\\nSample data:\")\n",
    "churn_df[to_show].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate target data\n",
    "churn_result = churn_df['Churn?']\n",
    "y = np.where(churn_result == 'True.',1,0)\n",
    "\n",
    "# We don't need these columns\n",
    "to_drop = ['State','Phone','Churn?']\n",
    "churn_feat_space = churn_df.drop(to_drop,axis=1)\n",
    "\n",
    "# 'yes'/'no' has to be converted to boolean values\n",
    "# NumPy converts these from boolean to 1. and 0. later\n",
    "yes_no_cols = [\"Int'l Plan\",\"VMail Plan\"]\n",
    "churn_feat_space[yes_no_cols] = churn_feat_space[yes_no_cols] == 'yes'\n",
    "\n",
    "# Pull out features for future use\n",
    "features = churn_feat_space.columns\n",
    "\n",
    "X = churn_feat_space.values.astype(np.float)\n",
    "\n",
    "print (\"Feature space holds %d observations and %d features\" % X.shape)\n",
    "print (\"Unique target labels:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 More about model selection: Cross-validation\n",
    "\n",
    "We saw in former pills that a nice way of assessing performance or comparing models is simulating the exploitation stage. Remember that this was done splitting the data set in training, validation and test sets. Because this splitting process has a randomness involved the resulting performance metric is also a random varible and the instantaneous value of the variable is heavily affected by the split. It can be the case that the split is very favorable and we get awesome performance metric values, or the other way around. We suggested the posibility of doing this same proces many times to get a good picture of the real behavior of the classifier. This is a perfectly correct way for assessing the performance. However one could argue that some points can be never chosen for testing, or that some points can be shared accross the different training splits. These could bias the result.\n",
    "\n",
    "Another well founded approach in order to circumvent the former problem is what we call **cross-validation**. The idea behind this process is that each point will be used for testing purposes. The most well known cross-validation techniques are:\n",
    "\n",
    "+ **Leave one out (LOO)**: Leave one out is as follows,\n",
    "    + Take one sample of the data set $x_i$.\n",
    "    + Train the classfier with all the data set except for the data selected $X_{train} = \\{X\\}\\setminus x_i$.\n",
    "    + Test the classifier on $x_i$ and store the result.\n",
    "    + Repeat the process for all samples of the data set.\n",
    "    + At the end of the process you should have an array with all the results ready for the computation of a performance metric.\n",
    "    \n",
    "Leave-one-out is computationally intensive, because it requires training a classifier as many times as examples in the data set we have. In order to alleviate this computational burden we can define the following process \n",
    "\n",
    "+ **K-fold cross-validation**: \n",
    "\n",
    "    + Split the data set in K disjoint subsets with the same cardinality, i.e.Â $\\{X\\} = S_1 \\cup S_2 \\dots \\cup S_k$ where $S_i \\subset \\{X\\}$, $S_i \\cap S_j = \\emptyset,  i\\neq j$, and $|S_i|\\approx |S_j|,  \\forall i, j$.\n",
    "    + Select one of the subsets $S_i$. This will be used as test set.\n",
    "    + Train the classfier in all except that subset, i.e. $X_{train} = \\{X\\} \\setminus S_i$.\n",
    "    + Test the trained classifier with $S_i$ and store the individual results for each sample in the subset (we can also consider the partial performance statistics in the subset for other hint on performance)\n",
    "    + Repeat for each subset\n",
    "    + At the end of the process you should have an array with all individual results ready for the computation of a performance metric. \n",
    "    \n",
    "This second approach is subject to some variability in the splitting process. Thus the resulting performance metric is again a random variable. One can repeat this process several times to estimate statistics such as the mean and variance of the classifier. Sometimes you will see partial performance statistics of each fold aggregated as simple way for approximating the process statistics.\n",
    "\n",
    "Leave one out can be seen as a particular instance of K-fold cross validation with K equal to the cardinality of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us practice this with the churn problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cv = model_selection.KFold(n_splits = 3 ,shuffle=True ,random_state=42)\n",
    "cv.get_n_splits(X)\n",
    "\n",
    "yhat = np.zeros((X.shape[0],1))\n",
    "for train_idx, test_idx in cv.split(X):\n",
    "    X_train,y_train = X[train_idx,:],y[train_idx]\n",
    "    X_test,y_test = X[test_idx,:],y[test_idx]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled=scaler.fit_transform(X_train)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = 11)\n",
    "    clf.fit(X_train_scaled,y_train)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    yhat[test_idx] = clf.predict(X_test_scaled).reshape(-1,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print ('Accuracy score: ' + str(metrics.accuracy_score(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\"><b>EXERCISE:</b> Let us check the performance for different values of `K = {2,3,5,10,20}`. In order to assess the variance of the process let us repeat each cross-validation `50` times. Show a box plot comparing the performances of the three methods.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.boxplot(acc)\n",
    "plt.gca().set_ylim([0.92,0.98])\n",
    "plt.title('Using the full set statistic')\n",
    "plt.subplot(122)\n",
    "plt.boxplot(partial_acc)\n",
    "plt.title('Using partial statistics')\n",
    "plt.gca().set_ylim([0.92,0.98])\n",
    "plt.gcf().set_size_inches(14,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Unbalanced datasets and the need for other kind of performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.pie(np.c_[len(y)-np.sum(y),np.sum(y)][0],labels=['No Churn','Churn'],colors=['r','g'],shadow=True,autopct ='%.2f' )\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(6,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "**Unbalanced datasets**\n",
    "<p>\n",
    "The unbalanced term describes the condition of the data where the ratio between the sizes of the positive and negative is a small value. In those scenarios, always predicting the majority class usually yields good accuracy performance, though it is ill informative. This kind of problems is very common when we want to model unusual events such as rare diseases, the occurrence of a failure in machinery, credit card fraud operations, etc. In those scenarios gathering data from usual events is very easy but collecting data from unusual events is difficult and results in a comparatively small size data set. In order to measure the performance on those data sets one has to use other performance metrics, such as specificity or positive predictive value on the minority class. In the end, the value of a misclassification of a sample depends on the application and the user. For example, in cancer detection because the cost of missing one patient in a trial is very large, we want the predictor to have very large sensitivity (we do not accept false negatives) though it means accepting more false positives. These false positives can be discarded in subsequent tests. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although accuracy is the most normal metric for evaluating classifiers, there are cases when the business value of correctly predicting elements from one class is different from the value for the prediction of elements of another class. In those cases, accuracy is not a good performance metric and more detailed analysis is needed. The **confusion matrix** enables us to define different metrics considering such scenarios. The confusion matrix considers the concepts of the classifier outcome and the actual ground truth or gold standard. In a binary problem, there are four possible cases: \n",
    "\n",
    "\n",
    "+ *True positives (TP):* When the classifier predicts a sample as positive and it really is positive.\n",
    "+ *False positives (FP):* When the classifier predicts a sample as positive but in fact it is negative.\n",
    "+ *True negatives (TN):* When the classifier predicts a sample as negative and it really is negative.\n",
    "+ *False negatives (FN):* When the classifier predicts a sample as negative but in fact it is positive.\n",
    "\n",
    "\n",
    "We can summarize this information in a matrix, namely the confusion matrix, as follows:\n",
    "<img src = \"./files/confmat.png\" width = 800px>\n",
    "The combination of these elements allows us to define several performance metrics:\n",
    "\n",
    "\n",
    "+ *Accuracy:*\n",
    "$$\\text{accuracy}=\\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{TN}+\\text{FP}+\\text{FN}}$$\n",
    "\n",
    "+ Column-wise we find these two partial performance metrics:\n",
    " + *Sensitivity or Recall:*\n",
    "$$\\text{sensitivity}=\\frac{\\text{TP}}{\\text{Real Positives}}=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$$\n",
    " + *Specificity:*\n",
    "$$\\text{specificity}=\\frac{\\text{TN}}{\\text{Real Negatives}}=\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}$$\n",
    "\n",
    "+ Row-wise we find these two partial performance metrics:}\n",
    "    + *Precision or Positive Predictive Value:*\n",
    "$$\\text{precision}=\\frac{\\text{TP}}{\\text{Predicted Positives}}=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$\n",
    "    + *Negative predictive value:*\n",
    "$$\\text{NPV}=\\frac{\\text{TN}}{\\text{Predicted Negative}}=\\frac{\\text{TN}}{\\text{TN}+\\text{FN}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class= \"alert alert-danger\" style = \"border-radius:10px\"> **QUIZ:** Consider the following questions and answer accordingly.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us code the confusion matrix and the different metrics for the churn problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import model_selection\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "clf.fit(X_train,y_train)\n",
    "yhat = clf.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = np.sum(np.logical_and(yhat==y_test,yhat==1))\n",
    "TN = np.sum(np.logical_and(yhat==y_test,yhat==0))\n",
    "FP = np.sum(np.logical_and(yhat!=y_test,yhat==1))\n",
    "FN = np.sum(np.logical_and(yhat!=y_test,yhat==0))\n",
    "\n",
    "print (TP,FP)\n",
    "print (FN,TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#I use wikipedia notation, thus we have to swap predictions and groundtruth\n",
    "print (metrics.confusion_matrix(yhat,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Another interesting metric is *F1-score*. This is defined as follows,\n",
    "\n",
    "$\\text{F1-score} = 2\\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision}+\\text{recall}} = 2\\frac{\\frac{TP}{TP+FP}\\frac{TP}{TP+FN}}{\\frac{TP}{TP+FP}+\\frac{TP}{TP+FN}} = \\frac{2 TP}{TP+FN+TP+FP} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style= \"border-radius:10px\">**QUESTION:** Why `classification_report` only reports precission and recall?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Operating point, ROC, and Area under the curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many classifiers can be considered as thresholded regressors. This means that the actual classifier actually reports some score that has to be further thresholded to finally decide the class. Consider, as an example, a binary classifier that scores the probability of belonging to class 'A', i.e. $P(x \\in 'A')$. Obviously, $1 - P(x \\in 'A')$ is the probability of belonging to class 'B'. The decision of belonging to class 'A' is given by $P(x \\in 'A')>thr$. We usually will use $thr = 0.5$ but we could change this threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style = \"border-radius:10px\">**QUESTION:** What is the effect of lowering the value of the threshold in the four partial performance metrics? Consider what happens to the positive samples ('A') and to the negative samples ('B')</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can the amount of true positive and the amount of false positives.This gives rise to the concept of the **operating point**. The operating point is the precise threshold we select for an specific application by controlling the true positive rate (recall) vs the false positive rate (1- specificity = FP / (FP+TN)). We can plot all the operating points by varying the threshold value and ploting the precission versus the recall. This curve is called **Receiver Operating Characteristic**. It shows how the true positive rate changes when the false positive rate change.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It also means that we can control precision and recall by modifying this threshold. \n",
    " This also allow us to define the **Precision-Recall curve**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 The perfect curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve tells us about the behavior of the positive class, displaying how the true positive rate changes as we change the false positive rate. Note that the curve depends on all the terms of the confusion matrix:\n",
    "\n",
    "\n",
    "$$TPR = sensitivity \\;(recall) = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "$$FPR = 1-specificity = \\frac{FP}{TN+FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style=\"border-radius:10px\"> **QUIZ** In the case of a perfect classification, what are the values of TPR and FRP ?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Computing the curve\n",
    "\n",
    "Let us compute the curve. In order to do so we need the score from a classifier. Some classifiers in sklearn have the method `predict_proba` that returns the \"confidence\" of the classification. Let us do this,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import model_selection\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=21)\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "clf.fit(X_train,y_train)\n",
    "yhat = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = np.sum(np.logical_and(yhat==y_test,yhat==1))\n",
    "TN = np.sum(np.logical_and(yhat==y_test,yhat==0))\n",
    "FP = np.sum(np.logical_and(yhat!=y_test,yhat==1))\n",
    "FN = np.sum(np.logical_and(yhat!=y_test,yhat==0))\n",
    "\n",
    "TPR = TP /(TP+FN)\n",
    "FPR = FP /(FP+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(TPR),np.sum(FPR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=np.argsort(score[:,1])\n",
    "\n",
    "TPR = []\n",
    "FPR = []\n",
    "for i in idx:\n",
    "    yhat = np.where(score[:,1]>score[i,1],1.0,0.0)\n",
    "    TP = np.sum(np.logical_and(yhat==y_test,yhat==1.))\n",
    "    TN = np.sum(np.logical_and(yhat==y_test,yhat==0.))\n",
    "    FP = np.sum(np.logical_and(yhat!=y_test,yhat==1.))\n",
    "    FN = np.sum(np.logical_and(yhat!=y_test,yhat==0.))\n",
    "\n",
    "    TPR.append( TP /(TP+FN) )\n",
    "    FPR.append( FP /(FP+TN) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(FPR)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(FPR)[idx],np.array(TPR)[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr,_ = metrics.roc_curve(y_test,score[:,1])\n",
    "fig = plt.figure()\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot(np.array(FPR)[idx],np.array(TPR)[idx],'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He,he! The same result as sklearn ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style = \"border-radius:10px\">**QUESTION:** What is the operating point?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Area under the curve\n",
    "\n",
    "The area under the curve is a good value for summarizing the ROC behavior. Although not perfect it gives a good idea for comparing classifiers. Let us compute the Area under the curve using the trapezoid method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_AUC(tpr,fpr):\n",
    "    val,idx = np.unique(fpr,return_index = True)\n",
    "    auc = 0\n",
    "    N = len(idx)-1 \n",
    "    for i in range(N):\n",
    "        auc  = auc + tpr[idx[i]]*(fpr[idx[i+1]]-fpr[idx[i]]) + (tpr[idx[i+1]]-tpr[idx[i]])*(fpr[idx[i+1]]-fpr[idx[i]])/2\n",
    "    auc = auc + tpr[idx[i+1]]*(1.-fpr[idx[i+1]]) + (1.-tpr[idx[i+1]])*(1.-fpr[idx[i+1]])/2\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_AUC(tpr,fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh!!!! Awesome, nearly the same ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Comparing classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare some classifiers in the problem of `Churn?` and check which one works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE: ** Compare a linear support vector machine, random forest, `11`-nearest neighbor, and logistic regression in the problem of Churn. \n",
    "<ol>\n",
    "<li>Split data in training and test, `test_size = 0.3`, `random_state = 0`.</li>\n",
    "<li>Draw in a single plot the four curves.</li>\n",
    "<li>Compute the area under the curve.</li>\n",
    "</ol>\n",
    "\n",
    "**HINT: ** In order to get the confidence/margin in the SVM use the method `decision_function`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The price of confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us work out a little more the problem of `Churn?`, once again. Consider the following simple business case:\n",
    "\n",
    "We want to lauch a retention marketing campaing:\n",
    "\n",
    "+ Each member in the subscription service gives us a profit of $100$ units.\n",
    "+ The cost of the campaign is $\\alpha=10$ units per advertisement.\n",
    "+ Supose that we only recover $\\beta = 10\\%$ of the people that received the campaign and were going to churn.\n",
    "+ We are going to use a classifier to select the targets of the campaign.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">\n",
    "**EXERCISE/QUESTION:** Model the former problem in terms of the elements of the confusion matrix. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">\n",
    "**EXERCISE:** Train a `LogisticRegressor` and check if the campaign is profitable. Use `test_size = 0.3` and `random_state = 31`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = c[1,1]\n",
    "FP = c[1,0]\n",
    "TN = c[0,0]\n",
    "FN = c[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN_hand = np.sum(np.logical_and(yhat!=y_test,yhat==0.))\n",
    "\n",
    "print (FN,FN_hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 10\n",
    "beta = 0.1\n",
    "balance=-alpha*(TP+FP)+beta*TP*100\n",
    "print (balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The campaign is not sustainable as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us choose an operating point, so that we maximize the validation profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = lr.predict_proba(X_test)\n",
    "\n",
    "idx=np.argsort(score[:,1])\n",
    "\n",
    "balance_old = -1e10 \n",
    "max_idx  = 0\n",
    "b = []\n",
    "for i in idx:\n",
    "    yhat = np.where(score[:,1]>score[i,1],1.0,0.0)\n",
    "    TP = np.sum(np.logical_and(yhat==y_test,yhat==1.))\n",
    "    TN = np.sum(np.logical_and(yhat==y_test,yhat==0.))\n",
    "    FP = np.sum(np.logical_and(yhat!=y_test,yhat==1.))\n",
    "    FN = np.sum(np.logical_and(yhat!=y_test,yhat==0.))\n",
    "    alpha = 10\n",
    "    beta = 0.1\n",
    "    balance=-alpha*(TP+FP)+beta*TP*100\n",
    "    \n",
    "    b.append(balance)\n",
    "    \n",
    "    if balance > balance_old:\n",
    "        max_idx = i\n",
    "        balance_old = balance\n",
    "    \n",
    "plt.plot(np.array(b))\n",
    "score[max_idx,1]\n",
    "yhat = np.where(score[:,1]>score[max_idx,1],1.0,0.0)\n",
    "print (metrics.confusion_matrix(yhat,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operating point indicates it is not worthwhile to consider a campaign with this kind of classifier. But can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dealing with unbalanced datasets\n",
    "\n",
    "The greatest problem we are facing is due to the fact we are dealing with unbalanced datasets and the original boundary just depends on that. We can try different things to balance the data set:\n",
    "\n",
    "+ Under sample the majority class.\n",
    "+ Over sample the minority class using some kind of data interpolator, for example SMOTE.\n",
    "+ Use class weights, this is also called cost-sensitive classification.\n",
    "+ Change the performance metric.\n",
    "+ Split the majority class in subclasses, then train as many classifiers as subclasses. Each involving one subclasses and the minority class. Then use an aggregation technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us proceed checking some of these techniques. Let us start with resampling the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pidx = np.where(y_train == 1)[0]\n",
    "nidx = np.where(y_train == 0)[0]\n",
    "\n",
    "print (np.sum(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidx=np.random.randint(0,len(nidx),size=np.sum(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_idx =[]\n",
    "resampled_idx=np.r_[pidx,sidx]\n",
    "X_resampled=X_train[resampled_idx,:]\n",
    "y_resampled=y_train[resampled_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "lr = linear_model.LogisticRegression()\n",
    "\n",
    "lr.fit(X_resampled,y_resampled)\n",
    "\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "c = metrics.confusion_matrix(yhat,y_test)\n",
    "\n",
    "\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = lr.predict_proba(X_test)\n",
    "\n",
    "idx=np.argsort(score[:,1])\n",
    "\n",
    "balance_old = -1e10 \n",
    "max_idx  = 0\n",
    "b = []\n",
    "for i in idx:\n",
    "    yhat = np.where(score[:,1]>score[i,1],1.0,0.0)\n",
    "    TP = np.sum(np.logical_and(yhat==y_test,yhat==1.))\n",
    "    TN = np.sum(np.logical_and(yhat==y_test,yhat==0.))\n",
    "    FP = np.sum(np.logical_and(yhat!=y_test,yhat==1.))\n",
    "    FN = np.sum(np.logical_and(yhat!=y_test,yhat==0.))\n",
    "    alpha = 10\n",
    "    beta = 0.1\n",
    "    balance=-alpha*(TP+FP)+beta*TP*100\n",
    "    \n",
    "    b.append(balance)\n",
    "    \n",
    "    if balance > balance_old:\n",
    "        max_idx = i\n",
    "        balance_old = balance\n",
    "    \n",
    "plt.plot(np.array(b))\n",
    "score[max_idx,1]\n",
    "yhat = np.where(score[:,1]>score[max_idx,1],1.0,0.0)\n",
    "print (metrics.confusion_matrix(yhat,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of addressing this problem is by assigning weights to the classes. Let us check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression(class_weight={1:0.0001})\n",
    "\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "c = metrics.confusion_matrix(yhat,y_test)\n",
    "\n",
    "\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = lr.predict_proba(X_test)\n",
    "\n",
    "idx=np.argsort(score[:,1])\n",
    "\n",
    "balance_old = -1e10 \n",
    "max_idx  = 0\n",
    "b = []\n",
    "for i in idx:\n",
    "    yhat = np.where(score[:,1]>score[i,1],1.0,0.0)\n",
    "    TP = np.sum(np.logical_and(yhat==y_test,yhat==1.))\n",
    "    TN = np.sum(np.logical_and(yhat==y_test,yhat==0.))\n",
    "    FP = np.sum(np.logical_and(yhat!=y_test,yhat==1.))\n",
    "    FN = np.sum(np.logical_and(yhat!=y_test,yhat==0.))\n",
    "    alpha = 10\n",
    "    beta = 0.1\n",
    "    balance=-alpha*(TP+FP)+beta*TP*100\n",
    "    \n",
    "    b.append(balance)\n",
    "    \n",
    "    if balance > balance_old:\n",
    "        max_idx = i\n",
    "        balance_old = balance\n",
    "    \n",
    "plt.plot(np.array(b))\n",
    "score[max_idx,1]\n",
    "yhat = np.where(score[:,1]>score[max_idx,1],1.0,0.0)\n",
    "print (metrics.confusion_matrix(yhat,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the best weight and model can be troublesome. Let us introduce our last methodology. We may use model selection checking for the parameters. There are different ways for doing so. The most well known is grid search on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "parameters = {'class_weight' : [{1:0.01},{1:0.1},{1:1},{1:10},{1:100}], 'C':[0.01,0.1,1.,10.,100.]}\n",
    "\n",
    "kf=model_selection.KFold(n_splits=5, shuffle=False, random_state=0)\n",
    "kf.get_n_splits(X)\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    #Standard parameters\n",
    "    clf = svm.LinearSVC()\n",
    "    # We can change the scoring \"average_precision\", \"recall\", \"f1\"\n",
    "    clf = model_selection.GridSearchCV(clf, parameters, scoring='precision_macro')\n",
    "    clf.fit(X_train,y_train.ravel())\n",
    "    X_test = scaler.transform(X_test)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    #recall, f1, precision\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    print (str(clf.best_params_))\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(yhat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style = \"border-radius:10px\">**DELIVERABLE: ** Next day we will work on the problem of `Churn?`. Report the best classifier found and their metrics.\n",
    "<p>\n",
    "Some hints\n",
    "<ol>\n",
    "<li>Select one or two classifiers. Some of the most powerful classifiers are `Random Forests`, `SVM with RBF kernel`, and `extreme Gradient Boosting`.</li>\n",
    "<li>Find out what parameters to validate. </li>\n",
    "<li>Grid search and cross validate the problem.</li>\n",
    "</ol>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
