{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>December 2016- This notebook was created by [Oriol Pujol Vila](http://www.maia.ub.es/~oriol). Source and license info are in the folder.</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are another kind of intutive classification strategy based on the divide and conquer paradigm. \n",
    "\n",
    "The basic **idea** in decision trees is to partition the space in patches and fit a model in that patch. There are two questions to answer in order to implement this solution:\n",
    "\n",
    "+ How do we partition the space?\n",
    "+ What model to use in each patch?\n",
    "\n",
    "In classification trees the second question is straight forward, each patch is given the value of a label and all data falling in that part of the space will be predicted as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Decision tree modeling\n",
    "\n",
    "Elements:\n",
    "\n",
    "- Splits using axis-orthogonal hyperplanes. This is the key that allows \"interpretability\" of the results.\n",
    "\n",
    "- At each internal node we test a value of a feature. A feature and a threshold are stored for each internal node.  \n",
    "\n",
    "- Leaves makes the class prediction. If leaves are pure, we have to store the class label. If leaves are impure, then the fraction of samples for each class is stored and its frequency is returned when queried.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Building our intuition on decision trees\n",
    "\n",
    "Let us build up our intuition with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see what the boundary looks like in a toy problem.\n",
    "%reset -f\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "MAXN=10\n",
    "np.random.seed(2)\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "idxplus = y==1\n",
    "idxminus = y==-1\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 15.0, delta)\n",
    "yy = np.arange(-5.0, 15.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.c_[Xf[:,np.newaxis],Yf[:,np.newaxis]];\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Tree\n",
    "import os\n",
    "dotfile = open(\"toy_tree.dot\", 'w')\n",
    "tree.export_graphviz(clf, out_file = dotfile)\n",
    "dotfile.close()\n",
    "\n",
    "os.system(\"dot -Tpng toy_tree.dot -o toy_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"toy_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the meaning of the tree. The first node splits the training set using feature $1$ by applying the threshold $\\leq 3.04$. As a result we are able to correctly classify eleven of the thirty data points. Let us see the boundary in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=0,max_depth=1)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second node splits the training set using feature $0$ by applying the threshold $\\leq 6.25$. Note that this only is used in the part of the space where feature $1$ is greater  than $3.04$. Observe that the remaining blue space is characterized by the following logical function: $(x_1>3.04) \\wedge (x_0\\leq 6.25)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is great about decision trees?\n",
    "\n",
    "+ Trees are easy for humans to interpret. It can be seen as a set of rules. Each path from root to one leaf of the tree is an AND combination of the thresholded features.\n",
    "+ Given a finite data set, decision trees can express any function of the input attributes. In ${\\bf R}^d$ we can isolate every point in the data set by constructing a box around each of them.\n",
    "+ There can be more than one tree that fits the same data. From all of them we would like a tree with minimum number of nodes. But the problem is NP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Learning the tree\n",
    "\n",
    "Because the problem is NP we can resort to a greedy construction algorithm. Greedy algorithms choose the current best binary partition without taking into account its impact on the quality of subsequent splits.\n",
    "\n",
    "The algorithm idea is as follows:\n",
    "\n",
    "+ Initialize the algorithm with a node associated to the full data set. \n",
    "\n",
    "**while** the list is not empty\n",
    "1. Retrieve the first node from the list.\n",
    "2. Find the data associated to that node.\n",
    "3. Find a splitting point.\n",
    "4. If the node is splittable, create the nodes linked to the parent node and put them in the exploration list.\n",
    "\n",
    "#### The splitting criterion\n",
    "\n",
    "There are many different splitting criteria. The most common ones are:\n",
    "\n",
    "+ Misclassification error\n",
    "+ Gini index\n",
    "+ Cross-entropy/Information gain/Mutual information\n",
    "\n",
    "Withouth going into details, misclassification error splits greedily select the split that corrects more data at each point. Gini index and cross-entropy probabilistically model the notion of impurity of a node. The split is chosen so that the average purity of the new nodes is maximized. Observe that as we descend in the tree the purity increases and eventually converge to pure leaves. A nice way of thinking about entropy is Pedro Domingos' simile with surprise. Entropy measures the average surprise/information a probabilistic result yields. In a binary variable, the maximum surprise occurs when both outcomes are equally probable, one has the maximum uncertainty on the result. Otherwise, the surprise decreases. This behavior is also display in Gini's index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "entropy = lambda p: -np.sum(p * np.log2(p)) if not 0 in p else 0\n",
    "gini = lambda p: 1. - (np.array(p)**2).sum()\n",
    "pvals = np.linspace(0, 1)        \n",
    "plt.plot(pvals, [entropy([p,1-p])/2. for p in pvals], label='Entropy')\n",
    "plt.plot(pvals, [gini([p,1-p]) for p in pvals], label='Gini')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees and overfitting\n",
    "\n",
    "Because trees are very expressive models they can model any training set perfectly and easily overfit.\n",
    "\n",
    "There are two ways of avoiding overfitting in trees:\n",
    "\n",
    "+ Stop growing the tree when the split is not statistically significant.\n",
    "+ Grow a full tree and post-prune.\n",
    "\n",
    "One of the simplest ways of post pruning is \"reduced error prunning\". It goes like this,\n",
    "\n",
    "1. Split data into training and validation\n",
    "2. Create a candidate tree on the training set\n",
    "3. Do until further pruning is harmful\n",
    "    1. Evaluate impact on the validation set of removing each posible node (with descendants)\n",
    "    2. Greedily remove the node that improves the performance the most.\n",
    "    \n",
    "Pruning is not implemented in sklearn at this moment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ensemble learning\n",
    "\n",
    "When we want to purchase a product we usually read user's reviews. Before undergoing a major surjery procedure we seek the opinion of different experts. Ensemble learning mimicks one of the human uncertainty reduction mechanism, seeking additional opinions before making a major decision.\n",
    "\n",
    "Ensemble learning is divided in two steps:\n",
    "\n",
    "1. Train a set of classifiers\n",
    "2. Aggregate their results\n",
    "\n",
    "There are different reasons for using ensemble learning in practice:\n",
    "\n",
    "1. **Statistical reasons:** The combination of outputs of different classifiers may reduce the risk of an unfortunate selection of a poorly performing classifier.\n",
    "2. **Large scale data sets:** It makes little sense to only have one classifier on very large sets of data. Partition data in smaller subsets and aggregate seems like a good idea.\n",
    "3. **Divide and conquer:** Some problems too difficult for a single classifier to solve. The decision boundary may be too complex or lie outside the space of functions of the classifier.\n",
    "4. **Data fusion:** Different source fusion is usually a problem. One usually faces data coming from heterogeneous sources and the question is how to fuse these data. One solution is to train one classifier per source and the fuse the decision of those experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity\n",
    "\n",
    "One condition required for the system to work is that errors on different classifiers should be made on different samples in order for the strategic combination of the classifiers to correct possible errors in the judgement of the class o a particular instance. This effect has been called **diversity**.\n",
    "\n",
    "Diversity can be obtained in different ways:\n",
    "\n",
    "+ Using different training sets. Use resampling strategies to obtain different optimal classifiers. This effect is correlated with the notion of stability of the classifier and the concept of bias and variance of the classifier.\n",
    "+ Using different training parameters for different classifiers\n",
    "+ Combining different architectures. (i.e. svm, decission trees, ...)\n",
    "+ Training on different features. (i.e. random subspaces or random projections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different kinds of ensemble models\n",
    "\n",
    "From a topological and architectural point of view, we can find three great ways for building an ensemble:\n",
    "\n",
    "- **Bagging:** Horitzontal aggregation. All elements trained outputs are combined using some aggregation rule. In bagging the elements combined are trained independently.\n",
    "- **Boosting:** Horitzontal aggregation. Each component in the aggregation depends on all the others. In this kind of techniques the two most well known approaches for linking members are the notions of residual (gradient boosting) or statistical resampling (adaptive boosting). \n",
    "- **Stacking:** Vertical aggregation. In this kind of techniques the output of one member of the ensemble is the input for the next member. For example, each layer in a neural network is vertically stacked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping aggregation \n",
    "\n",
    "Bootstrapping means resampling the training data set with replacement. Usually the same number of data as the original data set is used.\n",
    "\n",
    "**Bootstrapping aggregation (aka. Bagging)** is a ensemble technique that uses multiple bootstrapped copies of the training set to build a set of classifiers. One classifier for each bootstrapped training copy. And then, use a combination technique, such as majority voting, in order to take the final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check, how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us train an overfitted classifier. For example an SVC.\n",
    "%reset -f\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "MAXN=60\n",
    "np.random.seed(2)\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "idxplus = y==1\n",
    "idxminus = y==-1\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "x = np.linspace(-5,15,200)\n",
    "XX,YY = np.meshgrid(x,x)\n",
    "sz=XX.shape\n",
    "data=np.c_[XX.ravel(),YY.ravel()]\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "mx= np.max(Z)\n",
    "mn= np.min(Z)\n",
    "Z.shape=sz\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
    "plt.colorbar()\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bagged_tree(X,y,C):\n",
    "    clf_list=[]\n",
    "    for i in range(C):\n",
    "        np.random.seed(None)\n",
    "        idx=np.random.randint(0,y.shape[0],y.shape[0])\n",
    "        clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "        Xr=X[idx,:]\n",
    "        yr=y[idx]\n",
    "        clf_list.append((clf.fit(Xr,yr.ravel()),idx))  #Add the indices for visualization purposes in test\n",
    "    return clf_list\n",
    "\n",
    "\n",
    "def visualize_bagged_tree(X,y,clf_list):\n",
    "    C = len(clf_list)\n",
    "    x = np.linspace(-5,15,200)\n",
    "    XX,YY = np.meshgrid(x,x)\n",
    "    sz=XX.shape\n",
    "    data=np.c_[XX.ravel(),YY.ravel()]\n",
    "    yhat=np.zeros((data.shape[0],len(clf_list)))\n",
    "    i=0\n",
    "    for dt,idx in clf_list:\n",
    "        yhat[:,i]=dt.predict(data)\n",
    "        Xr=X[idx,:]\n",
    "        yr=y[idx]\n",
    "        mx= np.max(yhat[:,i])\n",
    "        mn= np.min(yhat[:,i])\n",
    "        plt.subplot(int(np.floor(C/4))+1,4,i+1)\n",
    "        plt.scatter(Xr[(yr==1).ravel(),0],Xr[(yr==1).ravel(),1],color='r')\n",
    "        plt.scatter(Xr[(yr==-1).ravel(),0],Xr[(yr==-1).ravel(),1],color='b')\n",
    "        plt.imshow(yhat[:,i].reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
    "        plt.contour(XX,YY,yhat[:,i].reshape(sz),[0])\n",
    "        i=i+1\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20,7*int(np.floor(C/4))+1)\n",
    "    return yhat\n",
    "\n",
    "\n",
    "clf_list=train_bagged_tree(X,y,16)\n",
    "y_pred=visualize_bagged_tree(X,y,clf_list)\n",
    "y_pred = np.sum(y_pred,axis=1)\n",
    "\n",
    "print ('Process finnished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx= np.max(y_pred)\n",
    "mn= np.min(y_pred)\n",
    "plt.figure()\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "plt.imshow(y_pred.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
    "plt.colorbar()\n",
    "plt.contour(XX,YY,y_pred.reshape(sz),[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zb = y_pred.reshape(sz)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "plt.imshow(Zb, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
    "plt.contour(XX,YY,Zb,[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application to customer churn prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check this approach in the Churn problem. Recall that a single decision tree achieved an accuracy of $91.7\\%$, precision of $71\\%$ and recall of $72\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#Recover Churn data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"churn_curated_numerical.csv\",header=None)\n",
    "df.head()\n",
    "data = df.values\n",
    "X = data[:,:-1]\n",
    "y = 2*data[:,-1]-1\n",
    "print ('Loading ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bagged_tree(X,y,C):\n",
    "    clf_list=[]\n",
    "    for i in range(C):\n",
    "        np.random.seed(None)\n",
    "        idx=np.random.randint(0,y.shape[0],y.shape[0])\n",
    "        clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "        Xr=X[idx,:]\n",
    "        yr=y[idx]\n",
    "        clf_list.append(clf.fit(Xr,yr.ravel()))  \n",
    "    return clf_list\n",
    "\n",
    "\n",
    "def test_bagged_tree(X,clf_list):\n",
    "    yhat=np.zeros((X.shape[0],len(clf_list)))\n",
    "    i=0\n",
    "    for dt in clf_list:\n",
    "        yhat[:,i]=dt.predict(X)\n",
    "        i=i+1\n",
    "    return np.sign(np.mean(yhat,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO SNOOPING\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "kf=model_selection.KFold(n_splits=5, shuffle=False, random_state=0)\n",
    "kf.get_n_splits()\n",
    "\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf_list = train_bagged_tree(X_train,y_train.ravel(),51)\n",
    "    yhat[test_index]=test_bagged_tree(X_test,clf_list) \n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm.T)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the solution accuracy increases by about $5\\%$, recall goes upt to $75\\%$ and precision also increases up to $91\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> **Bagging** performance improvement is due to the reduction of the variance of the classifier while mantaining its bias.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest technique introduces a randomization over the feature selected for building  each tree in the ensemble in order to improve diversity in an attempt to reduce variance evan more. Let us code this variant of bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "def train_random_forest(X,y,C):\n",
    "    F=int(np.ceil(np.sqrt(X.shape[1])))\n",
    "    clf_list=[]\n",
    "    for i in range(C):\n",
    "        np.random.seed(None)\n",
    "        idx=np.random.randint(0,y.shape[0],y.shape[0])\n",
    "        feat_idx=np.random.permutation(np.arange(X.shape[1]))[:10]\n",
    "        clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "        Xr=X[idx,:].copy()\n",
    "        Xr=Xr[:,feat_idx]\n",
    "        yr=y[idx]\n",
    "        clf_list.append((clf.fit(Xr,yr.ravel()),feat_idx))\n",
    "    return clf_list\n",
    "\n",
    "\n",
    "def test_random_forest(X,clf_list):\n",
    "    yhat=np.zeros((X.shape[0],len(clf_list)))\n",
    "    i=0\n",
    "    for dt,feat_idx in clf_list:\n",
    "        yhat[:,i]=dt.predict(X[:,feat_idx])\n",
    "        i=i+1\n",
    "    return np.sign(np.mean(yhat,axis=1)),yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "clf_list = train_random_forest(X,y,51)\n",
    "yhat,yk = test_random_forest(X,clf_list)\n",
    "acc = metrics.accuracy_score(yhat, y)\n",
    "print (yk.shape)\n",
    "print (np.sum(np.mean(yk,axis=1)>0))\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO SNOOPING\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "kf=model_selection.KFold(n_splits=5, shuffle=False, random_state=0)\n",
    "kf.get_n_splits()\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #dt = ensemble.RandomForestClassifier(n_estimators=51)\n",
    "    #dt.fit(X_train,y_train)\n",
    "    #yhat[test_index]=dt.predict(X_test)\n",
    "    clf_list = train_random_forest(X_train,y_train,51)\n",
    "    yhat[test_index],yk = test_random_forest(X_test,clf_list)\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print (acc)\n",
    "print (np.unique(yhat))\n",
    "print (np.unique(y_test))\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm.T)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental learning (stage-wise approximation) and boosting techniques\n",
    "\n",
    "+ Gradient boost\n",
    "+ Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now we have seen optimization techniques based on full step optimisation (e.g. gradient descent approaches). This is, we optimise all members of the ensemble at each step. We will now consider stage-wise approximations or incremental approximations. The most simple approach of this family is a greedy stage-wise approximation. In stage-wise approximations we consider the model up to our current time $t$ as fixed, and we just add and optimize a new element of the ensemble.\n",
    "\n",
    "Our model is\n",
    "\n",
    "$$H_t(x) = \\sum_{t=1}^T \\alpha_t h_t(x)$$\n",
    "\n",
    "Consider for example a least square approach \n",
    "\n",
    "$$\\mathcal{L}(x,y) = \\sum_{j=1}^N (y_j - \\sum_{t=1}^T \\alpha_t h_t(x_j))^2$$\n",
    "\n",
    "at time $t+1$, we have to select $h_{t+1}$ and optimize $\\alpha_{t+1}$ while keeping the model up to that point constant.\n",
    "\n",
    "$$H_{t+1}(x) = \\sum_{t=1}^T \\alpha_t h_t(x) + \\alpha_{t+1}h_{t+1}(x)$$\n",
    "\n",
    "If we plug this model into the loss \n",
    "\n",
    "$$\\mathcal{L}(x,y) = \\sum_{j=1}^N (\\underset{r_t(x_j)}{\\underbrace{y_j - \\sum_{t=1}^T \\alpha_t h_t(x_j)}} + \\alpha_{t+1}h_{t+1}(x_j))^2$$\n",
    "\n",
    "where $r_t(x_j) = y_j - H_t(x_j)$ is the error or residue commited by the model at time $t$ on example $x_j$.\n",
    "\n",
    "In this simple case, we have to solve for $\\alpha_{t+1}$ and $h_{t+1}$,\n",
    "\n",
    "$$\\nabla_{h_{t+1}} \\sum_{j=1}^N (r_t(x_j) + \\alpha_{t+1}h_{t+1}(x_j))^2 = 2\\sum_{j=1}^N (r_t(x_j) + \\alpha_{t+1}h_{t+1}(x_j))\\alpha_{t+1} = 0$$\n",
    "\n",
    "$$\\nabla_{\\alpha_{t+1}} \\sum_{j=1}^N (r_t(x_j) + \\alpha_{t+1}h_{t+1}(x_j))^2 = 2\\sum_{j=1}^N (r_t(x_j) + \\alpha_{t+1}h_{t+1}(x_j))h_{t+1}(x_j) = 0$$\n",
    "\n",
    "From the second equatino we can see that\n",
    "\n",
    "$$\\sum_{j=1}^N (r_t(x_j)h_{t+1}(x_j) + \\alpha_{t+1}h_{t+1}(x_j)h_{t+1}(x_j)) = $$\n",
    "$$r^T_t(x)h_{t+1}(x) + \\alpha_{t+1}h^T_{t+1}(x)h^T_{t+1}(x) = 0$$\n",
    "\n",
    "Observe that $\\alpha_{t+1}$ is proportional to the inner product of the current residue and the candidate function, \n",
    "\n",
    "$$\\alpha_{t+1} = \\frac{r^T_t(x)h_{t+1}(x)}{h^T_{t+1}(x)h_{t+1}(x)}$$\n",
    "\n",
    "In the case we have a binary problem $h^T_{t+1}(x) \\in \\{-1,+1\\}$, $h^T_{t+1}(x)h_{t+1}(x) = N$, the number of samples.\n",
    "\n",
    "In this case the selection of the optimum $h$ corresponds to the one that has minimum error.\n",
    "\n",
    "This is the core of **gradient boosting techniques**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular example of boosting is **Adaboost** (Adaptive boosting).\n",
    "\n",
    "In Adaboost we use the same idea but we change the loss function and use \n",
    "\n",
    "$$\\mathcal{L}(x,y) = \\sum_i e^{-y_iH(x_i)}$$\n",
    "\n",
    "In this case, let us drop the regularization term for reasons we will see afterwards. Thus, the problem to be optimized is,\n",
    "$$\n",
    "\t\\{\\alpha, h(x)\\}=\\underset{\\alpha, h(x)}{\\operatorname{arg\\,min}} \\sum_{i=1}^N e^{-y_i f(x_i)} = \\underset{\\alpha, h(x)}{\\operatorname{arg\\,min}} \\sum_{i=1}^N e^{-y_i \\sum_{j=1}^M \\alpha_j h_j(x_i)} \n",
    "$$\n",
    "We use a stage-wise approach as in the former algorithm.\n",
    "\n",
    "$$\n",
    "\\underset{\\alpha, h(x)}{\\operatorname{arg\\,min}} \\sum_{i=1}^N \\underset{w_{M-1}(x_i)}{\\underbrace{e^{-y_i \\sum_{j=1}^{M-1} \\alpha_j h_j(x_i)}}} e^{-y_i\\alpha_M h_M(x_i)}\n",
    "$$\n",
    "\n",
    "Divide in two parts : $h_M(x_i)=y_i$ and $h_M(x_i)\\neq y_i$ (Consider that $h_M(x_i)\\in\\{-1,1\\}$)\n",
    "\n",
    "Divide in two parts : $h_M(x_i)=y_i$ and $h_M(x_i)\\neq y_i$ (Consider that $h_M(x_i)\\in\\{-1,1\\}$)\n",
    "\n",
    "\\begin{align*}\n",
    " \\dots= \\underset{\\alpha, h(x)}{\\operatorname{arg\\,min}} \\sum_{i=1}^N w_{M-1}(x_i) e^{- y_i \\alpha_M h_M(x_i)} = \\\\\n",
    " = e^{-\\alpha_M}\\sum_{y_i=h_M(x_i)} w_{M-1}(x_i) + e^{\\alpha_M}\\sum_{y_i\\neq h_M(x_i)} w_{M-1}(x_i) = \\\\\n",
    " = (e^{\\alpha_M} - e^{-\\alpha_M}) \\sum_{i=1}^N w_{M-1}(x_i) I(y_i \\neq h_M(x_i)) + e^{-\\alpha_M} \\sum_{i=1}^N w_{M-1}(x_i)\n",
    " \\end{align*}\n",
    "\n",
    " \n",
    " Setting the gradient of the learning function to zero and recalling the derived expression (simplified notation ${\\bf I} = I(y_i \\neq h_M(x_i))$): \n",
    " \n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\alpha_M}\\big ( (e^{\\alpha_M} - e^{-\\alpha_M}) \\sum_{i=1}^N w_{M-1}(x_i) {\\bf I} + e^{-\\alpha_M} \\sum_{i=1}^N w_{M-1}(x_i) \\big )= 0\\\\\n",
    "(e^{\\alpha_M} + e^{-\\alpha_M})\\sum_i w_{M-1}{\\bf I} - e^{-\\alpha_M} \\sum_i w_{M-1}=0\\\\\n",
    "(e^{\\alpha_M} + e^{-\\alpha_M})\\frac{\\sum_i w_{M-1}{\\bf I}}{\\sum_i w_{M-1}} - e^{-\\alpha_M} =0\n",
    "\\end{align*}\n",
    "Observe that $\\frac{\\sum_i w_{M-1}{\\bf I}}{\\sum_i w_{M-1}}$ is the error (${\\bf err}_{M-1}$) at stage $M-1$.\n",
    "\n",
    "$$\n",
    "(e^{\\alpha_M} + e^{-\\alpha_M}) {\\bf err}_{M-1} - e^{-\\alpha_M} =0\n",
    "$$\n",
    "\n",
    "Simple manipulation allows us to isolate $\\alpha_M$:\n",
    "\n",
    "$$\n",
    "\\alpha_M = \\frac{1}{2} log \\big ( \\frac{1- {\\bf err}_{M-1} }{ {\\bf err}_{M-1} } \\big )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the algorithm is as follows:\n",
    "\n",
    "1. Initialize $w_0(x_i)=1/N.$\n",
    "2. **for** $m=1\\dots M:$\n",
    "    + Find the function $h_m(x)$ that minimizes the weighted error $$h_m(x)=\\underset{h}{\\operatorname{arg\\,min}}\\sum_i w_{m-1}(x_i) I(h_m(x_i)\\neq y_i)$$\n",
    "    + Compute the weighted error $$ {\\bf err}_{m-1}=\\frac{\\sum_i w_{m-1}{\\bf I}}{\\sum_i w_{m-1}}$$.\n",
    "    + Choose $$\\alpha_m=\\frac{1}{2}\\ln\\big(\\frac{1-{\\bf err}_{m-1}}{{\\bf err}_{m-1}}\\big)$$\n",
    "    + Update the weights:\n",
    "$$\n",
    "w_m(x_i)=\\frac{w_{m-1}(x_i) e^{- y \\alpha_m h_m(x_i)}}{Z_m}\n",
    "$$\n",
    "where $Z_m=\\sum_i w_m(x_i)$ is a normalization factor.\n",
    "\n",
    "    + Update model: $$f_{m}(x)=f_{m-1}(x)+\\alpha_{m}h_m(x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reductionist frameworks for the multi-class problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this moment we have applied several models in multi-class problems but we have barely talked about the problem of multiple classes in learning. \n",
    "\n",
    "First of all, there are very few models that intrinsically handle the multi-class case. By intrinsically handling the multi-class problem I am referring to methods in which we do not have to worry about how many classes our problem has. The two big families of models that can deal with the problem are\n",
    "\n",
    "+ Decision trees: the leaves encode the class.\n",
    "+ Nearest Neighbors: we only care about class labels of instances close to my query sample.\n",
    "\n",
    "What about the rest of the models? Did not Bayesian models or Neural Networks also handle this problem? Yes, they work in the multi-class case. But we have to worry about how many classes there are. In particular we have to build a model for each class and then take a maximum score/probability/confidence among the predictions. This way of addressing the multiclass problem is also known as **one-against-all** because we consider one model for each class while the samples from the rest of the classes are considered as negative samples. This is the first example of a reductionist framework.\n",
    "\n",
    "The reductionist framework refers to those ensemble methods that allows to reduce the multi-class problem to a set of binary problems. In a $K$ class problem, the two most common approaches in this framework are:\n",
    "\n",
    "+ **one-against-all:** We consider $K$ partitions of the problem, corresponding to setting one class as positive class and the rest as negative. \n",
    "+ **one-against-one:** We consider all posible pairs of classes and build a model for each subproblem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#Create a multiclass toy problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "MAXN=5\n",
    "np.random.seed(0)\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.25*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,-2]+1.25*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,3*np.ones((MAXN,1))])\n",
    "\n",
    "#Display data\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r',label='class 1')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b',label='class 2')\n",
    "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g',label='class 3')\n",
    "plt.legend()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "\n",
    "#Train a LinearSVC in one-vs-all fashion\n",
    "clf_list=[]\n",
    "for i in range(3):\n",
    "    clf = svm.LinearSVC()\n",
    "    y_meta = y.copy()\n",
    "    #Create a binary problem with one class at +1 and the rest at -1\n",
    "    y_meta=np.where(y_meta == i+1 ,1,-1)\n",
    "    clf_list.append(clf.fit(X,y_meta.ravel()))\n",
    "\n",
    "#Test each classifier\n",
    "plt.figure()\n",
    "x = np.linspace(-5,15,200)\n",
    "XX,YY = np.meshgrid(x,x)\n",
    "sz=XX.shape\n",
    "data=np.c_[XX.ravel(),YY.ravel()]\n",
    "i=1\n",
    "yhat_d=np.empty((data.shape[0],3))\n",
    "for c in clf_list:\n",
    "    yhat=c.predict(data)\n",
    "    #Visualization of each boundary\n",
    "    yhat_d[:,i-1]=c.decision_function(data)\n",
    "    mn = np.min(yhat)\n",
    "    mx = np.max(yhat)\n",
    "    plt.subplot(1,3,i)\n",
    "    plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='b',label='class 1')\n",
    "    plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='g',label='class 2')\n",
    "    plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='r',label='class 3')\n",
    "    plt.imshow(yhat.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
    "    plt.contour(XX,YY,yhat.reshape(sz),[0])\n",
    "    i=i+1\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,9)\n",
    "\n",
    "y_final=np.argmax(yhat_d,axis=1)\n",
    "plt.figure()\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='b',label='class 1')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='g',label='class 2')\n",
    "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='r',label='class 3')\n",
    "plt.imshow(y_final.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=2)\n",
    "plt.contour(XX,YY,y_final.reshape(sz),[0, 1])\n",
    "plt.title(\"Final decision boundary\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Error correcting output coding\n",
    "\n",
    "Error correcting output coding is a generalization of the methods shown before. In the most general case each class is assigned a ternary code $c_i \\in \\{+1,0,-1\\}^l$ with length $l$. This step is called **coding**. In testing a new sample will be given a test code and this will be compared according to some distance to the class codewords. The class with the closest codeword will be selected as the predicted class. This step is called **decoding**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Understanding the coding step\n",
    "If we arrange the codewords as rows in a matrix we obtain the coding matrix $M \\in \\{+1,0,-1\\}^{K\\times l}$. Consider the following example with four classes and code length $l=3$:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>$h_1$</th>\n",
    "    <th>$h_2$</th>\n",
    "    <th>$h_3$</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>$y_1$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$1$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_2$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$0$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_3$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$0$</th>\n",
    "    <th>$1$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_4$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$0$</th>\n",
    "    <th>$-1$</th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The first class, $y_1$ is coded as $(1,1,1)$, the second $y_2$ is coded as $(1,-1,0)$, and so on.\n",
    "\n",
    "Note that the columns of the matrix define a binary problem involving all the classes in the following way: in same column, all classes with code $+1$ belongs to the same meta-class, all classes with code $-1$ to the other meta-class, and all classes with code $0$ are not considered in that particular problem. In our example, the first column defines a binary problem involving the discrimination of all samples from classes $y_1,y_2$ (coded as $+1$) against all the samples of $y_3,y_4$ (coded as $-1$). The second column only considers the samples of class $y_1$ against the samples of class $y_2$. Note that all the zero coded classes are not considered. \n",
    "\n",
    "Given a coding matrix a classifier is trained for each column according to the column defined binary problem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">**EXERCISE:** Which are the coding matrix of one-vs-one and one-vs-all?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Understanding the decoding step\n",
    "\n",
    "Given a set of classifiers trained according to the problems defined by the columns of the coding matrix, in the prediction step all the classifiers are applyed to the testing sample. As a result a binary code $t$ is obtained. This coded is compared to all the class codes according to some decoding/distance metric. The most common ones are:\n",
    "\n",
    "+ Hamming decoding/$\\ell_1$-decoding\n",
    "$$d(a,b) = \\frac{1}{2}\\sum\\limits_{i=1}^l |a_i-b_i|$$\n",
    "\n",
    "+ Euclidean decoding\n",
    "$$d(a,b) = \\sqrt{\\sum\\limits_{i=1}^l (a_i-b_i)^2}$$\n",
    "\n",
    "For example, consider that $t=(-1,-1,-1)$. Note that there is no exact code in the coding matrix, thus we have to check for the closest one. If we apply Hamming decoding we obtain\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Hamming</th>\n",
    "    <th>Euclidean</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>$y_1$</th>\n",
    "    <th>$3$</th>\n",
    "    <th>$\\sqrt{12}$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_2$</th>\n",
    "    <th>$\\frac{3}{2}$</th>\n",
    "    <th>$\\sqrt{5}$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_3$</th>\n",
    "    <th>$\\frac{3}{2}$</th>\n",
    "    <th>$\\sqrt{5}$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_4$</th>\n",
    "    <th>$\\frac{1}{2}$</th>\n",
    "    <th>$1$</th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Observe that in both cases the sample will be predicted as class $y_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply this framework to the former problem using a one-vs-all approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#Create a multiclass toy problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "MAXN=5\n",
    "np.random.seed(0)\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.25*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,-2]+1.25*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([0*np.ones((MAXN,1)),1*np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,2*np.ones((MAXN,1))])\n",
    "x = np.linspace(-5,15,200)\n",
    "XX,YY = np.meshgrid(x,x)\n",
    "sz=XX.shape\n",
    "data=np.c_[XX.ravel(),YY.ravel()]\n",
    "\n",
    "#Define the coding matrix\n",
    "M = np.array([[1, -1, -1],[-1, 1, -1],[-1, -1, 1]]) #1vsAll\n",
    "#M = np.array([[1, 1, 0 ],[-1, 0, 1],[0, -1, -1]]) #1vs1\n",
    "\n",
    "print ('Coding matrix M = \\n' + str(M))\n",
    "def inset(a,b): \n",
    "    return [item in b for item in a]\n",
    "\n",
    "def fit_ECOC(X, y, M):\n",
    "    clf_list=[]\n",
    "    for i in range(M.shape[1]): #For each column\n",
    "        y_meta=y.copy()\n",
    "        idx_c1 = np.where(inset(y, np.where(M[:,i]==1)[0]))[0]\n",
    "        idx_c2 = np.where(inset(y, np.where(M[:,i]==-1)[0]))[0]\n",
    "        clf = svm.LinearSVC()\n",
    "        clf_list.append(clf.fit(np.r_['0',X[idx_c1,:],X[idx_c2,:]],np.r_['0',np.ones((idx_c1.shape[0],1)),-np.ones((idx_c2.shape[0],1))].ravel()))\n",
    "    return clf_list\n",
    "\n",
    "def predict_ECOC(X,M, clf_list):\n",
    "    #Test codes\n",
    "    c = np.zeros((X.shape[0],M.shape[1]))\n",
    "    for i in range(M.shape[1]):\n",
    "        c[:,i]=clf_list[i].decision_function(X) #SOFT CODES\n",
    "        #c[:,i]=clf_list[i].predict(X) #HARD CODES \n",
    "    #Use Euclidean distance\n",
    "    i=0\n",
    "    d = np.zeros((X.shape[0],M.shape[0]))\n",
    "    for code in M:\n",
    "        d[:,i]=np.sum(np.power((c-code),2),axis=1)\n",
    "        i=i+1\n",
    "    return np.argmin(d,axis=1)    \n",
    "\n",
    "\n",
    "clf_list=fit_ECOC(X,y,M)\n",
    "y_final = predict_ECOC(data,M,clf_list)\n",
    "\n",
    "plt.scatter(X[(y==0).ravel(),0],X[(y==0).ravel(),1],color='b',label='class 1')\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='g',label='class 2')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='r',label='class 3')\n",
    "plt.imshow(y_final.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=2)\n",
    "plt.contour(XX,YY,y_final.reshape(sz),[0, 1])\n",
    "plt.title(\"Final decision boundary\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**EXERCISE:** Replace the hard codes prediction by the soft coding by changing in the ECOC_predict() function the \".predict\" by \".decision_function\". Run the algorithm with one-against-all approach. What do you observe?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**EXERCISE:** Replace the coding matrix by one-vs-one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[1, 1, 0 ],[-1, 0, 1],[0, -1, -1]])\n",
    "clf_list=fit_ECOC(X,y,M)\n",
    "y_final2 = predict_ECOC(data,M,clf_list)\n",
    "\n",
    "plt.scatter(X[(y==0).ravel(),0],X[(y==0).ravel(),1],color='b',label='class 1')\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='g',label='class 2')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='r',label='class 3')\n",
    "plt.imshow(y_final2.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=2)\n",
    "\n",
    "plt.imshow(y_final.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=2)\n",
    "plt.contour(XX,YY,y_final2.reshape(sz),[0, 1])\n",
    "plt.title(\"Final decision boundary\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Bonus content] Extreme learning machines or whatever..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a simple random projection followed by a non-linearity and a straight forward least squares optimization. Let us code that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Create data\n",
    "x,y = make_moons(noise=0.1)\n",
    "y=2.*y-1.\n",
    "plt.scatter(x[y==1,0],x[y==1,1],color='b',label='class 1')\n",
    "plt.scatter(x[y==-1,0],x[y==-1,1],color='r',label='class 2')\n",
    "\n",
    "\n",
    "#Code xlm\n",
    "layer1_size = 600\n",
    "dim = 2\n",
    "lam = 0.5\n",
    "W_in = np.random.normal(size=(dim+1,layer1_size)) \n",
    "z_in = np.dot(x,W_in[:-1,:])+W_in[-1,:]\n",
    "z_out = np.where(z_in<0,0,z_in)\n",
    "z_out = np.c_[z_out,np.ones((z_out.shape[0],1))]\n",
    "den = np.linalg.inv(np.dot(z_out.T,z_out)+lam*np.eye(z_out.shape[1]))\n",
    "num = np.dot(z_out.T,y)\n",
    "W_out=np.dot(den,num)\n",
    "\n",
    "def eval_xlm(W_in,W_out,data):\n",
    "    z_in = np.dot(data,W_in[:-1,:])+W_in[-1,:]    \n",
    "    z_out = np.where(z_in<0,0,z_in)\n",
    "    z_out = np.c_[z_out,np.ones((z_out.shape[0],1))]\n",
    "    return np.dot(z_out,W_out)\n",
    "\n",
    "eval_xlm(W_in,W_out,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x[y==1,0],x[y==1,1],color='r',label='class 1')\n",
    "plt.scatter(x[y==-1,0],x[y==-1,1],color='b',label='class 2')\n",
    "xlin = np.linspace(-2,3,200)\n",
    "XX,YY = np.meshgrid(xlin,xlin)\n",
    "sz=XX.shape\n",
    "data=np.c_[XX.ravel(),YY.ravel()]\n",
    "print (data.shape)\n",
    "y_final = eval_xlm(W_in,W_out,data)\n",
    "print (y_final.shape)\n",
    "plt.imshow(y_final.reshape(sz), interpolation='bilinear', origin='lower', extent=(-2,3,-2,3),alpha=0.3, vmin=0, vmax=2)\n",
    "plt.contour(XX,YY,y_final.reshape(sz),[0])\n",
    "plt.title(\"Final decision boundary\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
