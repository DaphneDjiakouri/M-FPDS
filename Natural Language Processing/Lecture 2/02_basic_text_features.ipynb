{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:32:38.496531Z",
     "start_time": "2023-02-22T16:32:31.431877Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:32:38.503126Z",
     "start_time": "2023-02-22T16:32:38.499623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.__version__=1.3.2\n"
     ]
    }
   ],
   "source": [
    "print(f'sklearn.__version__={sklearn.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:33:09.276088Z",
     "start_time": "2023-02-22T16:33:09.272616Z"
    }
   },
   "outputs": [],
   "source": [
    "# spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter    = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:33:11.674799Z",
     "start_time": "2023-02-22T16:33:11.667250Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                PorterStemmer       LancasterStemmer    \n",
      "\n",
      "dogs                dog                 dog                 \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n",
      "pass                pass                pass                \n",
      "passing             pass                pass                \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "passed              pass                pass                \n",
      "trouble             troubl              troubl              \n",
      "troubling           troubl              troubl              \n",
      "care                care                car                 \n",
      "believes            believ              believ              \n"
     ]
    }
   ],
   "source": [
    "words = [\"dogs\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\n",
    "         \"football\",\"pass\",\"passing\",\"friendship\", \"friends\", \"friendships\",\n",
    "         \"passed\",\"trouble\",\"troubling\",\"care\", \"believes\"]\n",
    "preprocess = [porter, lancaster]\n",
    "\n",
    "len_bin = 20\n",
    "col_formater = \"{0:len_bin}{1:len_bin}{2:len_bin}\".replace(\"len_bin\",str(len_bin))\n",
    "print(col_formater.format(\"Word\", porter.__class__.__name__, lancaster.__class__.__name__))\n",
    "print(\"\")\n",
    "for w in words:\n",
    "    print( col_formater.format(w, porter.stem(w), lancaster.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:40:13.735208Z",
     "start_time": "2022-02-23T16:40:13.730729Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def stem(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_stemmed = []\n",
    "    for word in token_words:\n",
    "        sentence_stemmed.append(porter.stem(word))\n",
    "        sentence_stemmed.append(\" \")\n",
    "    return \"\".join(sentence_stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:40:14.169873Z",
     "start_time": "2022-02-23T16:40:14.165343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j.k. rowl wrote harri potter . she never expect the book to be famou . '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"J.K. Rowling wrote Harry Potter. She never expected the book to be famous.\"\n",
    "stem(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T16:35:16.898070Z",
     "start_time": "2021-03-01T16:35:16.893744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J.K. Rowling wrote Harry Potter.',\n",
       " 'She never expected the book to be famous.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:09.482102Z",
     "start_time": "2021-03-01T12:00:09.479536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J',\n",
       " 'K',\n",
       " ' Rowling wrote Harry Potter',\n",
       " ' She never expected the book to be famous',\n",
       " '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Be carefull separating phrases\n",
    "s.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:09.487212Z",
     "start_time": "2021-03-01T12:00:09.484157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J.K.',\n",
       " 'Rowling',\n",
       " 'wrote',\n",
       " 'Harry',\n",
       " 'Potter',\n",
       " '.',\n",
       " 'She',\n",
       " 'never',\n",
       " 'expected',\n",
       " 'the',\n",
       " 'book',\n",
       " 'to',\n",
       " 'be',\n",
       " 'famous',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Lemmatization\n",
    "\n",
    "\n",
    "Lemmatization consists on properly use of a vocabulary and morphological analysis of words, aiming to remove inflectional endings only with the goal of returning any word to a set of base (or dictionary form) words.\n",
    "\n",
    "\n",
    "`Lemmatize(saw) = see`\n",
    "\n",
    "\n",
    "We will use a lemmatizer from WordNet (https://wordnet.princeton.edu) avaliable from nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:40:42.197929Z",
     "start_time": "2022-02-23T16:40:42.193460Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:40:55.586339Z",
     "start_time": "2022-02-23T16:40:55.581486Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\35796/nltk_data'\n    - 'c:\\\\Users\\\\35796\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\35796\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\35796\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\35796\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI was running and eating. This was a terrible idea.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m punctuations\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?:!.,;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m sentence_words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence_words:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m punctuations:\n",
      "File \u001b[1;32mc:\\Users\\35796\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\35796\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\35796\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\35796\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\35796\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\35796/nltk_data'\n    - 'c:\\\\Users\\\\35796\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\35796\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\35796\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\35796\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I was running and eating. This was a terrible idea.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:40:56.025899Z",
     "start_time": "2022-02-23T16:40:55.999563Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the words did no change!\n",
    "\n",
    "This is because there was no context. If we give a part of speech type then the lemmatizer will do what we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T16:37:19.701471Z",
     "start_time": "2021-03-01T16:37:19.698730Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T16:37:27.522658Z",
     "start_time": "2021-03-01T16:37:27.517302Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = [\"dogs\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\n",
    "         \"football\",\"pass\",\"passing\",\"friendship\", \"friends\", \"friendships\",\n",
    "         \"passed\",\"trouble\",\"troubling\",\"care\", \"believes\"]\n",
    "preprocess = [porter, lancaster, wordnet_lemmatizer]\n",
    "\n",
    "len_bin = 20\n",
    "col_formater = \"{0:len_bin}{1:len_bin}{2:len_bin}{3:len_bin}\".replace(\"len_bin\",str(len_bin))\n",
    "print(col_formater.format(\"Word\", porter.__class__.__name__, lancaster.__class__.__name__, wordnet_lemmatizer.__class__.__name__))\n",
    "print(\"\")\n",
    "for w in words:\n",
    "    print( col_formater.format(w, porter.stem(w), lancaster.stem(w), wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Features for documents\n",
    "\n",
    "\n",
    "### From docs to feature vectors: Make your own countvectorizer\n",
    "\n",
    "\n",
    "Let us build a simple document classifier featurizing each document by word counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:34:31.365925Z",
     "start_time": "2023-02-22T16:34:31.359038Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.datasets\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:34:35.498431Z",
     "start_time": "2023-02-22T16:34:34.244104Z"
    }
   },
   "outputs": [],
   "source": [
    "X = sklearn.datasets.fetch_20newsgroups()\n",
    "\n",
    "X_train = sklearn.datasets.fetch_20newsgroups(subset=\"train\").data\n",
    "y_train = sklearn.datasets.fetch_20newsgroups(subset=\"train\").target\n",
    "X_test  = sklearn.datasets.fetch_20newsgroups(subset=\"test\").data\n",
    "y_test  = sklearn.datasets.fetch_20newsgroups(subset=\"test\").target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:34:46.343463Z",
     "start_time": "2023-02-22T16:34:46.339969Z"
    }
   },
   "outputs": [],
   "source": [
    "x = X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:43:06.035635Z",
     "start_time": "2022-02-23T16:43:06.028683Z"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:42:37.383950Z",
     "start_time": "2022-02-23T16:42:37.379237Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:11.545231Z",
     "start_time": "2021-03-01T12:00:11.542259Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiny function to create a feature matrix of word counts (feature counting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "docs = [['hello', 'world', 'hello'], ['goodbye', 'cruel', 'teacher', 'goodbye']]\n",
    "\n",
    "def prepare_word_counts_with_dict(docs: Iterable[str], verbose=False):\n",
    "    ind_ptr = [0]\n",
    "    ind_col = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    \n",
    "    for m, doc in enumerate(docs):\n",
    "        word_ind_counter = defaultdict(int)  # document counter for each doc in X\n",
    "        for word in doc: \n",
    "            vocabulary.setdefault(word, len(vocabulary))\n",
    "            word_ind_counter[word] += 1\n",
    "                \n",
    "        data.extend(word_ind_counter.values())\n",
    "        ind_ptr.append(ind_ptr[-1] + len(word_ind_counter))\n",
    "        ind_col.extend([vocabulary[w] for w in word_ind_counter.keys()])\n",
    "    \n",
    "    if verbose:\n",
    "        print(data)\n",
    "        print(ind_col)\n",
    "        print(ind_ptr)\n",
    "    return (data, ind_col, ind_ptr)\n",
    "\n",
    "sp.csr_matrix(prepare_word_counts_with_dict(docs, verbose=True)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [['hello', 'world', 'hello'], ['goodbye', 'cruel', 'teacher', 'goodbye']]\n",
    "\n",
    "def prepare_word_counts_with_dict(docs: Iterable[str], verbose=False):\n",
    "    ind_ptr = [0]\n",
    "    ind_col = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    \n",
    "    for m, doc in enumerate(docs):\n",
    "        word_ind_counter = defaultdict(int)  # document counter for each doc in X\n",
    "        for word in doc: \n",
    "            vocabulary.setdefault(word, len(vocabulary))\n",
    "            word_ind_counter[word] += 1\n",
    "                \n",
    "        data.extend(word_ind_counter.values())\n",
    "        ind_ptr.append(ind_ptr[-1] + len(word_ind_counter))\n",
    "        ind_col.extend([vocabulary[w] for w in word_ind_counter.keys()])\n",
    "\n",
    "    if verbose:\n",
    "        print('len vocab =', len(vocabulary))\n",
    "        print('vocab =', vocabulary)\n",
    "        print('data =', data)\n",
    "        print('ind_ptr =', ind_ptr)\n",
    "        print('ind_col =', ind_col)\n",
    "        \n",
    "    return (data, ind_col, ind_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prepare_word_counts_with_dict(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sp.csr_matrix(prepare_word_counts_with_dict(docs)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a bigger dataset to benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_big = docs * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "prepare_word_counts_with_dict(docs_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.csr_matrix(prepare_word_counts_with_dict(docs_big))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiny function to create a feature matrix of word counts  (no feature counting)\n",
    "\n",
    "We can create a CSR word count matrix without explicitly counting each word count.\n",
    "\n",
    "- **Note**: `sp.csr_matrix` is smart enough to join counts of `data`, `ind_col` and `ind_ptr` that happen to be in the same coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [['hello', 'world', 'hello'], ['goodbye', 'cruel', 'teacher']]\n",
    "\n",
    "def prepare_word_counts0(docs: Iterable[str]):\n",
    "    ind_ptr = [0]\n",
    "    ind_col = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    \n",
    "    for doc in docs:\n",
    "        for w in doc:\n",
    "            if w in vocabulary:\n",
    "                index = vocabulary[w]\n",
    "            else:\n",
    "                index = len(vocabulary)\n",
    "                vocabulary[w] = len(vocabulary)\n",
    "            ind_col.append(index)\n",
    "            data.append(1)\n",
    "        ind_ptr.append(len(ind_col))\n",
    "    return (data, ind_col, ind_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, ind_col, ind_ptr = prepare_word_counts0(docs)\n",
    "print(data)\n",
    "print(ind_col)\n",
    "print(ind_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.csr_matrix(prepare_word_counts0(docs)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "docs = [['hello', 'world', 'hello'], ['goodbye', 'cruel', 'teacher']]\n",
    "\n",
    "def prepare_word_counts1(docs: Iterable[str]):\n",
    "    ind_ptr = [0]\n",
    "    ind_col = []\n",
    "    data = []\n",
    "    vocabulary = defaultdict(int)\n",
    "    \n",
    "    for doc in docs:\n",
    "        for w in doc:\n",
    "            index = vocabulary.setdefault(w, len(vocabulary))\n",
    "            ind_col.append(index)\n",
    "            data.append(1)\n",
    "        ind_ptr.append(len(ind_col))\n",
    "    \n",
    "    return (data, ind_col, ind_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.csr_matrix(prepare_word_counts(docs)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking  approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_big = docs*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "prepare_word_counts0(docs_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "prepare_word_counts1(docs_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "prepare_word_counts_with_dict(docs_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customising Vectorizer classes\n",
    "\n",
    "- **preprocessor**: a callable that takes an entire document as input (as a single string), and returns a possibly transformed version of the document, still as an entire string. This can be used to remove HTML tags, lowercase the entire document, etc.\n",
    "\n",
    "\n",
    "- **tokenizer**: a callable that takes the output from the preprocessor and splits it into tokens, then returns a list of these.\n",
    "\n",
    "\n",
    "- **analyzer**: a callable that replaces the preprocessor and tokenizer. The default analyzers all call the preprocessor and tokenizer, but custom analyzers will skip this. N-gram extraction and stop word filtering take place at the analyzer level, so a custom analyzer may have to reproduce these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of how to encode sparse matrix fast\n",
    "\n",
    "\n",
    "Notice that in order to build our data as a matrix we need to use sparse matrices due to the high dimensionality (number of words/features) of the vocabulary.\n",
    "\n",
    "Here there is a little example to illustrate how we can build a csr_matrix (compressed sparse row matrix) fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:44:11.920221Z",
     "start_time": "2022-02-23T16:44:11.902757Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,0,2],[2,1,0],[0,1,3]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:44:12.339149Z",
     "start_time": "2022-02-23T16:44:12.332451Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [1,2,2,1,1,3]\n",
    "row  = [0,0,1,1,2,2]\n",
    "#ind_ptr = [0,3,4,9]\n",
    "col = [0,2,0,1,1,2]\n",
    "sp.csr_matrix( (data,(row,col)), shape=(3,3) ).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Build a Simple countvectorizer\n",
    "\n",
    "Complete methods `fit` and `transform`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:44:15.377045Z",
     "start_time": "2022-02-23T16:44:15.372815Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[4], y_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:11.585220Z",
     "start_time": "2021-03-01T12:00:11.583601Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"David's car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:52:20.013586Z",
     "start_time": "2022-02-23T16:52:19.991490Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "stemmer =  SnowballStemmer(language='english')\n",
    "\n",
    "class SimpleCountVectorizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 min_word_counts=1,\n",
    "                 doc_cleaner_pattern=r\"[^a-zA-Z]\",\n",
    "                 token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 dtype=np.float32,\n",
    "                 doc_cleaner_func=None,\n",
    "                 tokenizer_func=None,\n",
    "                 word_transformer_func=None):\n",
    "        \n",
    "        self._retype = type(re.compile('hello, world'))\n",
    "\n",
    "        self.min_word_counts     = min_word_counts\n",
    "        self.doc_cleaner_pattern = doc_cleaner_pattern\n",
    "        self.token_pattern       = token_pattern\n",
    "        self.dtype               = dtype\n",
    "        \n",
    "        self.doc_cleaner_func      = doc_cleaner_func\n",
    "        self.tokenizer_func        = tokenizer_func\n",
    "        self.word_transformer_func = word_transformer_func\n",
    "\n",
    "        self.word_to_pos = {}\n",
    "\n",
    "\n",
    "    def build_doc_cleaner(self, lower=True):\n",
    "        \"\"\"\n",
    "        Returns a function that cleans undesirable substrings in a string.\n",
    "        It also lowers the input string if lower=True\n",
    "        \"\"\"\n",
    "        if self.doc_cleaner_func:\n",
    "            return self.doc_cleaner_func\n",
    "        else:\n",
    "            if isinstance(self.doc_cleaner_pattern, self._retype):\n",
    "                clean_doc_pattern = re.compile(self.doc_cleaner_pattern)\n",
    "            else:\n",
    "                clean_doc_pattern = re.compile(self.doc_cleaner_pattern)\n",
    "\n",
    "            if lower:\n",
    "                 return lambda doc: clean_doc_pattern.sub(\" \", doc).lower()\n",
    "            else:\n",
    "                 return lambda doc: clean_doc_pattern.sub(\" \", doc)\n",
    "\n",
    "    def build_tokenizer(self):\n",
    "        \"\"\"Returns a function that splits a string into a sequence of tokens\"\"\"\n",
    "        if self.tokenizer_func:\n",
    "            return self.tokenizer_func\n",
    "        \n",
    "        else:\n",
    "            token_pattern = re.compile(self.token_pattern)\n",
    "            return lambda doc: token_pattern.findall(doc)\n",
    "\n",
    "    def build_word_transformer(self):\n",
    "        \"\"\"Returns a stemmer or lemmatizer if object has any\"\"\"\n",
    "        \n",
    "        if self.word_transformer_func:\n",
    "            return self.word_transformer_func\n",
    "        else:\n",
    "            return lambda word: word\n",
    "        \n",
    "    def tokenize(self, doc):\n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        doc     = doc_cleaner(doc)\n",
    "        words = doc_tokenizer(doc)\n",
    "            \n",
    "        return words\n",
    "        \n",
    "    def fit(self, X):\n",
    "\n",
    "        assert isinstance(X,list), \"X is expected to be a list of documents\"\n",
    "        \n",
    "        i = 0\n",
    "        word_to_pos = {}\n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        word_transformer = self.build_word_transformer()\n",
    "        \n",
    "        for x in X:\n",
    "            x     = doc_cleaner(x)\n",
    "            words = doc_tokenizer(x)\n",
    "            for word in words:\n",
    "                word = word_transformer(word)                  \n",
    "                if word not in word_to_pos:\n",
    "                    word_to_pos[word] = i\n",
    "                    i = i + 1\n",
    "\n",
    "        #self.doc_cleaner = doc_cleaner\n",
    "        #self.doc_tokenizer = doc_tokenizer\n",
    "        #self.word_transformer = word_transformer\n",
    "        \n",
    "        self.word_to_pos = word_to_pos            \n",
    "        self.n_features = len(self.word_to_pos)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Implements a transform where counts are created at runtime and kept with a dict\n",
    "        \"\"\"\n",
    "        \n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        word_transformer = self.build_word_transformer()      \n",
    "        \n",
    "        col_indices = []\n",
    "        row_indices = []\n",
    "        sp_data     = []\n",
    "        \n",
    "        for m, doc in enumerate(X):\n",
    "            doc = doc_cleaner(doc)\n",
    "            word_ind_counter = defaultdict(int)  # document counter for each doc in X\n",
    "            for word in doc_tokenizer(doc):\n",
    "                word = word_transformer(word)   \n",
    "                if word in self.word_to_pos:\n",
    "                    word_ind_counter[self.word_to_pos[word]] +=1 # word count aggregation\n",
    "\n",
    "            sp_data.extend(word_ind_counter.values())\n",
    "            row_indices.extend([m]*len(word_ind_counter))\n",
    "            col_indices.extend(word_ind_counter.keys())\n",
    "\n",
    "        encoded_X = sp.csr_matrix((sp_data,(row_indices,col_indices)),\n",
    "                                   shape=(len(X), self.n_features),\n",
    "                                   dtype=self.dtype)\n",
    "        \n",
    "        return encoded_X\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        word_to_pos = {}\n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        word_transformer = self.build_word_transformer()\n",
    "        \n",
    "        data = []\n",
    "        ind_col = []\n",
    "        ind_ptr = [0]\n",
    "        \n",
    "        for x in X:\n",
    "            x     = doc_cleaner(x)\n",
    "            words = doc_tokenizer(x)\n",
    "            for word in words:\n",
    "                word = word_transformer(word)                  \n",
    "                index = word_to_pos.setdefault(word, len(word_to_pos))\n",
    "                ind_col.append(index)\n",
    "                data.append(1)\n",
    "            ind_ptr.append(len(ind_col))\n",
    "                           \n",
    "        self.word_to_pos = word_to_pos            \n",
    "        self.n_features = len(self.word_to_pos)\n",
    "        self.word_to_pos = word_to_pos\n",
    "        \n",
    "        #self.doc_cleaner = doc_cleaner\n",
    "        #self.doc_tokenizer = doc_tokenizer\n",
    "        #self.word_transformer = word_transformer\n",
    "        \n",
    "        X_transformed = sp.csr_matrix((data, ind_col, ind_ptr))\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training a document classifier with `SimpleCountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:40.019110Z",
     "start_time": "2021-03-01T12:00:38.615255Z"
    }
   },
   "outputs": [],
   "source": [
    "vainilla_count_vectorizer = SimpleCountVectorizer( doc_cleaner_func=lambda doc: doc)\n",
    "vainilla_count_vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:42.991870Z",
     "start_time": "2021-03-01T12:00:40.020675Z"
    }
   },
   "outputs": [],
   "source": [
    "vainilla_count_vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:01:26.948126Z",
     "start_time": "2021-03-01T12:00:45.573090Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_ = vainilla_count_vectorizer.transform(X_train)\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1, max_iter=50)\n",
    "logistic.fit(X_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:01:26.999629Z",
     "start_time": "2021-03-01T12:01:26.949918Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(logistic.predict(X_train_) == y_train.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I) No Stemmer and no doc_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:38:48.175414Z",
     "start_time": "2023-02-22T16:38:48.170122Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vainilla_count_vectorizer = CountVectorizer()\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_0 = sklearn.pipeline.Pipeline([(\"countvectorizer\", vainilla_count_vectorizer),\n",
    "                                         (\"logisticregression\", logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:39:53.238353Z",
     "start_time": "2023-02-22T16:39:53.231420Z"
    }
   },
   "outputs": [],
   "source": [
    "model_pipe_0.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:40:44.274826Z",
     "start_time": "2023-02-22T16:40:42.774106Z"
    }
   },
   "outputs": [],
   "source": [
    "model_pipe_0.fit(X_train[0:100],y_train[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:41:04.622712Z",
     "start_time": "2023-02-22T16:41:04.593545Z"
    }
   },
   "outputs": [],
   "source": [
    "model_pipe_0.predict(X_train[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:06:09.444697Z",
     "start_time": "2021-03-01T12:05:20.635740Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_pipe_0.fit(X_train,y_train)\n",
    "y_test_pred  = model_pipe_0.predict(X_test)\n",
    "y_train_pred = model_pipe_0.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:06:13.725989Z",
     "start_time": "2021-03-01T12:06:11.087461Z"
    }
   },
   "outputs": [],
   "source": [
    "model_pipe_0.steps[0][1].transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:06:15.394958Z",
     "start_time": "2021-03-01T12:06:15.392235Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_train_0 = np.mean(y_train == y_train_pred)\n",
    "acc_test_0 = np.mean(y_test == y_test_pred)\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_0, acc_test_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II) No stemmer but doc_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:06:24.979150Z",
     "start_time": "2021-03-01T12:06:24.976884Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer = SimpleCountVectorizer(doc_cleaner_pattern=re.compile(\"[^a-zA-Z]\"))\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_1 = sklearn.pipeline.Pipeline([(\"countvectorizer\", simple_count_vectorizer),\n",
    "                                        (\"logisticregression\", logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:06:52.680919Z",
     "start_time": "2021-03-01T12:06:26.928292Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_pipe_1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:07:00.535058Z",
     "start_time": "2021-03-01T12:06:54.528964Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred  = model_pipe_1.predict(X_test)\n",
    "y_train_pred = model_pipe_1.predict(X_train)\n",
    "\n",
    "acc_train_1 = np.mean(y_train == y_train_pred)\n",
    "acc_test_1 = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_1, acc_test_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III) Use a SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:07:06.207058Z",
     "start_time": "2021-03-01T12:07:06.204178Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer_stemmer = SimpleCountVectorizer(word_transformer_func= SnowballStemmer('english').stem,\n",
    "                                                        doc_cleaner_pattern=re.compile(\"[^a-zA-Z]\"))\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_2 = sklearn.pipeline.Pipeline([(\"countvectorizer\", simple_count_vectorizer_stemmer),\n",
    "                                        (\"logisticregression\", logistic)],\n",
    "                                         )#memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:08:46.522104Z",
     "start_time": "2021-03-01T12:07:08.034092Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_pipe_2.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_2.predict(X_test)\n",
    "y_train_pred = model_pipe_2.predict(X_train)\n",
    "\n",
    "acc_train_2 = np.mean(y_train == y_train_pred)\n",
    "acc_test_2  = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_2, acc_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table with results for each pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:08:52.509004Z",
     "start_time": "2021-03-01T12:08:52.222112Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:08:54.275249Z",
     "start_time": "2021-03-01T12:08:54.270254Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results[\"no clean no stem\"]   = [acc_train_0, acc_test_0]\n",
    "df_results[\"yes clean no stem\"]  = [acc_train_1, acc_test_1]\n",
    "df_results[\"yes clean yes stem\"] = [acc_train_2, acc_test_2]\n",
    "df_results.index=[\"train\",\"test\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:08:56.076926Z",
     "start_time": "2021-03-01T12:08:56.065436Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###   Ngram features with Sklearn vectorizer\n",
    "\n",
    "\n",
    "####  IV) Training a document classifier with sklearn `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:08:57.897420Z",
     "start_time": "2021-03-01T12:08:57.895297Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_3 = sklearn.pipeline.Pipeline([(\"countvectorizer\", count_vectorizer),\n",
    "                                          (\"logisticregression\", logistic)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:09:41.755666Z",
     "start_time": "2021-03-01T12:08:59.659210Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_pipe_3.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_3.predict(X_test)\n",
    "y_train_pred = model_pipe_3.predict(X_train)\n",
    "\n",
    "acc_train_3 = np.mean(y_train == y_train_pred)\n",
    "acc_test_3  = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_3, acc_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:09:45.428569Z",
     "start_time": "2021-03-01T12:09:45.426135Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results[\"sklearn countvectorizer\"] = [acc_train_3, acc_test_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:09:47.234189Z",
     "start_time": "2021-03-01T12:09:47.228044Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V) Training a document classifier with sklearn `CountVectorizer` and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:13:51.960008Z",
     "start_time": "2021-03-01T12:13:51.875889Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2))\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_4 = sklearn.pipeline.Pipeline([(\"countvectorizer\", count_vectorizer),\n",
    "                                          (\"logisticregression\", logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:19:03.957095Z",
     "start_time": "2021-03-01T12:13:53.865132Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_pipe_4.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_4.predict(X_test)\n",
    "y_train_pred = model_pipe_4.predict(X_train)\n",
    "\n",
    "acc_train_4 = np.mean(y_train == y_train_pred)\n",
    "acc_test_4  = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_4, acc_test_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:20:13.081738Z",
     "start_time": "2021-03-01T13:20:13.078679Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_pipe_4.steps[0][1].transform(X_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:20:15.111395Z",
     "start_time": "2021-03-01T13:20:15.108945Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results[\"sklearn countvectorizer 2gram\"] = [acc_train_4, acc_test_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:20:17.001315Z",
     "start_time": "2021-03-01T13:20:16.995125Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:20:19.301620Z",
     "start_time": "2021-03-01T13:20:18.913165Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df_results.T[\"test\"].plot(kind=\"barh\", xlim=(0.79,0.83))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SelectKbest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:20:24.297308Z",
     "start_time": "2021-03-01T13:20:24.294423Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2))\n",
    "feature_selector = SelectKBest(chi2, k = 700000)\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_5 = sklearn.pipeline.Pipeline([(\"count_vectorizer\", count_vectorizer),\n",
    "                                          (\"feature_selector\", feature_selector),\n",
    "                                          (\"logisticregression\", logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:23:49.440495Z",
     "start_time": "2021-03-01T13:20:26.279867Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_pipe_5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:23:59.516522Z",
     "start_time": "2021-03-01T13:23:51.327069Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_train = np.mean(model_pipe_5.predict(X_train) == y_train)\n",
    "acc_test = np.mean(model_pipe_5.predict(X_test) == y_test)\n",
    "df_results[\"sklearn countvectorizer 2gram + selection\"] = [acc_train, acc_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:24:01.492699Z",
     "start_time": "2021-03-01T13:24:01.382054Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_results.T[\"test\"].plot(kind=\"barh\", xlim=(0.79,0.83))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:24:03.354145Z",
     "start_time": "2021-03-01T13:24:03.351799Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer_stemmer = SimpleCountVectorizer(word_transformer_func= SnowballStemmer('english').stem,\n",
    "                                                        doc_cleaner_pattern=re.compile(\"[^a-zA-Z]\"))\n",
    "\n",
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:24:05.228472Z",
     "start_time": "2021-03-01T13:24:05.226390Z"
    }
   },
   "outputs": [],
   "source": [
    "union = sklearn.pipeline.FeatureUnion([(\"simple_count_vectorizer_stemmer\", simple_count_vectorizer_stemmer),\n",
    "                                       (\"count_vectorizer\", count_vectorizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:24:07.133052Z",
     "start_time": "2021-03-01T13:24:07.130039Z"
    }
   },
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "feature_selector = SelectKBest(chi2, k = 700000)\n",
    "model_pipe_6 = sklearn.pipeline.Pipeline([(\"union_vectorizers\", union),\n",
    "                                          (\"feature_selector\", feature_selector),\n",
    "                                          (\"logisticregression\", logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:28:00.134193Z",
     "start_time": "2021-03-01T13:24:09.065420Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_pipe_6.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:29:01.299014Z",
     "start_time": "2021-03-01T13:28:02.436371Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_train = np.mean(model_pipe_6.predict(X_train) == y_train)\n",
    "acc_test = np.mean(model_pipe_6.predict(X_test) == y_test)\n",
    "df_results[\"Feature union + selection\"] = [acc_train, acc_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:29:03.333324Z",
     "start_time": "2021-03-01T13:29:03.209539Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_results.T[\"test\"].plot(kind=\"barh\", xlim=(0.79,0.83))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
