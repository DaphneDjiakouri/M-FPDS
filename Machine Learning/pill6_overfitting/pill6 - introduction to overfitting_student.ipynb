{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Pill 6- Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style = \"border-radius:10px\">**DEPENDENCIES:** `ipywidgets` dependency.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lecture we saw that the process of learning consists of finding the model such that \n",
    "\n",
    "$$E_{out}\\rightarrow 0$$\n",
    "\n",
    "This problem can not directly be addressed. For this reason in order to achieve this we decompose it in two different processes.\n",
    "\n",
    "Because we have data we can operate upon, we can use a proxy of the out-of-sample error, the empirical or in-sample error $E_{in}$. \n",
    "\n",
    "Thus, we would like two conditions\n",
    "\n",
    "$$E_{in} \\rightarrow 0$$\n",
    "\n",
    "but also that \n",
    "\n",
    "$$E_{out}\\approx E_{in}$$.\n",
    "\n",
    "The first condition is achieved by selecting an optimization/learning method that minimizes $E_{in}$ or a surrogate function. The second condition is met when we consider the probabilistic setting and derive bounds by means of Hoeffding's inequality and variants. \n",
    "\n",
    "In that sense, we saw that\n",
    "\n",
    "$$E_{out}\\leq E_{in} + \\mathcal{O}(\\sqrt{\\frac{C}{N}})$$\n",
    "where $C$ is a notion of complexity and $N$ the amount of data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us practice a little with these notions and observe the consequences.\n",
    "\n",
    "Let me define a polynomial using Chebyshev's polynomial basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def polyval(coefs, x):\n",
    "    res = coefs[0]*np.ones((1,x.shape[0]))\n",
    "    for i in range(1,len(coefs)):\n",
    "        res = res + coefs[i]*np.power(x,i)\n",
    "    return res\n",
    "        \n",
    "\n",
    "def chebys (coefs, x):\n",
    "    pol = {}\n",
    "    pol[0] = [1][::-1]\n",
    "    pol[1] = [1, 0][::-1]\n",
    "    pol[2] = [2, 0, -1][::-1]\n",
    "    pol[3] = [4, 0, -3, 0][::-1]\n",
    "    pol[4] = [8, 0, -8, 0, 1][::-1]\n",
    "    pol[5] = [16, 0, -20, 0, 5, 0][::-1]\n",
    "    pol[6] = [32, 0, -48, 0, 18, 0, -1][::-1]\n",
    "    pol[7] = [64, 0, -112, 0, 56, 0, -7, 0][::-1]\n",
    "    pol[8] = [128, 0, -256, 0, 160, 0, -32, 0, 1][::-1]\n",
    "    pol[9] = [256, 0, -576, 0, 432, 0, -120, 0, 9, 0][::-1]\n",
    "    pol[10] = [512, 0, -1280, 0, 1120, 0, -400, 0, 50, 0, -1][::-1]\n",
    "    pol[11] = [1024, 0, -2816, 0, 2816, 0, -1232, 0, 220, 0, -11, 0][::-1]\n",
    "\n",
    "    res = np.zeros((1,x.shape[0]))\n",
    "\n",
    "    for i in range(len(coefs)):\n",
    "        res= res + coefs[i]*polyval(pol[i],x)\n",
    "    return res\n",
    "\n",
    "        \n",
    "x = np.linspace(0,1,100)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(11):\n",
    "    co = np.zeros((11,))\n",
    "    co[i]=1.\n",
    "    plt.figure()\n",
    "    plt.plot(x,chebys(co,x).ravel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a random polynomial and add some noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a polynomial with noise\n",
    "np.random.seed(0)\n",
    "coefs = np.random.random(11)\n",
    "\n",
    "x = np.linspace(0,1,100)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "vals=chebys(coefs,x).ravel()\n",
    "#plt.plot(x,vals)\n",
    "\n",
    "N=15\n",
    "idx = np.random.randint(0,100,N)\n",
    "data = vals[idx]+0.5*np.random.normal(size=len(idx))\n",
    "\n",
    "plt.plot(x[idx],data,'ro')\n",
    "\n",
    "x_train = x[idx]\n",
    "y_train = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style = \"border-radius:10px\">**QUIZ/EXERCISE** \n",
    "We have a data set that comes from a 10th order polynomial using Chebishev polynomial basis with some noise added. \n",
    "\n",
    "<ol>\n",
    "<li>Fit a tenth order polynomial using what you know about linear regression and interaction features. **HINT:** Remember that for each sample you can create the powers of the value up to order 10 and then use that to fit a linear model.</li>\n",
    "<li>Compute the in-sample root mean squared error of the fit.</li>\n",
    "<li>Compute an approximation of the out of sample root mean squared error by using 100 uniform out of samples.</li>\n",
    "</ol>\n",
    "<p>\n",
    "**HINT:** Use a preconditioner of 1e-6 in the matrix inversion. This is \n",
    "\n",
    "$$A^{-1} \\approx (A+10^{-6}I)^{-1}$$\n",
    "\n",
    "where $I$ is the identity matrix.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style = \"border-radius:10px\">**QUIZ** \n",
    "Repeat the former exercise for polynomial degrees 3. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a polynomial without noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us repeat the problem using without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a polynomial with noise\n",
    "np.random.seed(0)\n",
    "coefs = np.random.random(11)\n",
    "\n",
    "x = np.linspace(0,1,100)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "vals=chebys(coefs,x).ravel()\n",
    "plt.plot(x,vals)\n",
    "\n",
    "N=10\n",
    "idx = np.random.randint(0,100,N)\n",
    "data = vals[idx]\n",
    "\n",
    "plt.plot(x[idx],data,'ro')\n",
    "\n",
    "x_train = x[idx]\n",
    "y_train = data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style = \"border-radius:10px\">**POLL:** \n",
    "We have the sample from a 10th-order polynomial and we know that the true generating function is a 10th-order polynomial and that we don't have noise. What model order should I use?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style = \"border-radius:10px\">**QUIZ/EXERCISE** \n",
    "We have the data set that comes from a 10th order polynomial using Chebishev polynomial basis without noise added. Compute the in-sample RMSE and out-of-sample RMSE for polynomial fits with degree 10 and 3.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE** \n",
    "Repeat the exercise with N = 30. Compute the in-sample RMSE and out-of-sample RMSE for polynomial fits with degree 10 and 3. What do we observe now?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a polynomial with noise\n",
    "np.random.seed(42)\n",
    "coefs = np.random.random(11)\n",
    "\n",
    "x = np.linspace(0,1,100)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "vals=chebys(coefs,x).ravel()\n",
    "plt.plot(x,vals)\n",
    "\n",
    "N=30\n",
    "idx = np.random.randint(0,100,N)\n",
    "data = vals[idx]\n",
    "\n",
    "plt.plot(x[idx],data,'ro')\n",
    "\n",
    "x_train = x[idx]\n",
    "y_train = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TAKE HOME IDEA:** Independently of the data generation process generating we have to match the data complexity and not the true model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to see what is going on with a classification example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from sklearn import neighbors\n",
    "\n",
    "MAXC=50\n",
    "MAXN=1000\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "perm = np.random.permutation(y.size)\n",
    "X = X[perm,:]\n",
    "y = y[perm]\n",
    "\n",
    "\n",
    "def complexity_number(C,N):\n",
    "    \n",
    "    Xr=X[:N,:]\n",
    "    yr=y[:N]\n",
    "    idxplus = yr==1\n",
    "    idxminus = yr==-1\n",
    "    idxplus = idxplus.flatten()\n",
    "    idxminus = idxminus.flatten()\n",
    "    plt.scatter(Xr[idxplus,0],Xr[idxplus,1],color='r')\n",
    "    plt.scatter(Xr[idxminus,0],Xr[idxminus,1],color='b')   \n",
    "    delta = 0.05\n",
    "    xx = np.arange(-5.0, 15.0, delta)\n",
    "    yy = np.arange(-5.0, 15.0, delta)\n",
    "    XX, YY = np.meshgrid(xx, yy)\n",
    "    Xf = XX.flatten()\n",
    "    Yf = YY.flatten()\n",
    "    sz=XX.shape\n",
    "    data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "    #Evaluate the model for a given weight\n",
    "    clf = neighbors.KNeighborsClassifier(MAXC-C+1)\n",
    "    clf.fit(Xr,yr.ravel())\n",
    "    Z=clf.predict(data)\n",
    "    Z.shape=sz\n",
    "    plt.scatter(Xr[idxplus,0],Xr[idxplus,1],color='r')\n",
    "    plt.scatter(Xr[idxminus,0],Xr[idxminus,1],color='b')\n",
    "    plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "    plt.contour(XX,YY,Z,[0])\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)\n",
    "   \n",
    "#Ipython 2.0\n",
    "interact(complexity_number,  C=(1,MAXC), N = (20,MAXN));\n",
    "#Ipython 1.1.\n",
    "#C=1 #1...50\n",
    "#N = 20 #20...1000\n",
    "#complexity_number(C,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE: ** \n",
    "<li> Set the number of data samples per cluster $N$ to $100$ and the complexity value $C$ to $50$. Describe what you observe: Does the method missclassify any data sample? \n",
    "<li> Decrease the complexity value to $C = 20$. Describe the boundary: Does the method missclassify any data sample?\n",
    "<li> Which of the two settings do you think will perform the best in front of new data from the same distribution? Why?\n",
    "<li> Increase the number of data points to $N = 1000$ with $C=50$. Describe what you observe. Will the method perform better than the same method with $N=100$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the behavior observed. For this purpose we may draw a curve of the training error and test error as the number of training data increases for a given complexity. This curve is called **learning curve**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "C=5\n",
    "MAXN=1000\n",
    "\n",
    "yhat_test=np.zeros((10,299,2))\n",
    "yhat_train=np.zeros((10,299,2))\n",
    "#Repeat ten times to get smooth curves\n",
    "for i in range(10):\n",
    "    X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "    X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "    y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "    y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "    perm = np.random.permutation(y.size)\n",
    "    X = X[perm,:]\n",
    "    y = y[perm]\n",
    "\n",
    "    X_test = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "    X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "    y_test = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "    y_test = np.concatenate([y_test,np.ones((MAXN,1))])\n",
    "    j=0\n",
    "    for N in range(10,3000,10):\n",
    "        Xr=X[:N,:]\n",
    "        yr=y[:N]\n",
    "        idxplus = yr==1\n",
    "        idxminus = yr==-1\n",
    "        idxplus = idxplus.flatten()\n",
    "        idxminus = idxminus.flatten()\n",
    "        #Evaluate the model\n",
    "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
    "        clf.fit(Xr,yr.ravel())\n",
    "        yhat_test[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
    "        yhat_train[i,j,0] = 1. - metrics.accuracy_score(clf.predict(Xr), yr.ravel())\n",
    "        j=j+1\n",
    "\n",
    "plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),'r')\n",
    "plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'b')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,5)\n",
    "plt.xlabel('Number of samples x10')\n",
    "plt.ylabel('Error rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the former plot we can see:\n",
    "\n",
    "+ As the number of training samples increase both errors tends to the same value, **bias**.\n",
    "+ When we have a little amount of training data, training error is very small but test error is very large.\n",
    "\n",
    "Check now the learning curve when the complexity is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE** \n",
    "Repeat the exercise with C = 1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a similar behavior in this second curve. Let us compare the two plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1,=plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),color='pink')\n",
    "p2,=plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'c')\n",
    "p3,=plt.plot(np.mean(yhat_test[:,:,1].T,axis=1),'r')\n",
    "p4,=plt.plot(np.mean(yhat_train[:,:,1].T,axis=1),'b')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,5)\n",
    "plt.xlabel('Number of samples x10')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend([p1,p2,p3,p4],[\"Test C = 5\",\"Train C = 5\",\"Test C = 1\",\"Train C = 1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although both show a similar behavior we note several differences:\n",
    "\n",
    "+ With small complexity training and test errors converge sooner/with a smaller amount of data.\n",
    "+ However, with small complexity, the error of converge is larger than with increased complexity.\n",
    "\n",
    ">The value towards both errors converge is also called **bias**, and the difference between this value and the test error is called **variance**. The **bias/variance** decomposition of the learning curve is an alternative view to the training and generalization view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression setting we can easily characterize the **bias/variance** decomposition. Consider our target function $g(x)$, and the observed data $y = g(x) + \\eta$, where $\\eta$ is a random variable representing noise with $\\mathbb{E}[\\eta] = 0$. Then we want to characterize the expected squared error\n",
    "\n",
    "$$\\mathbb{E}_{\\mathcal{D},x}[(y-f(x))^2]$$\n",
    "\n",
    "We may decompose this as follows:\n",
    "\n",
    "$$\\mathbb{E}_{\\mathcal{D},x}[(y-f(x))^2] = \\mathbb{E}_{\\mathcal{D},x}[(g(x)+\\eta-f(x))^2]\n",
    "$$\n",
    "\n",
    "we can now add and substract the hypothesis mean $\\bar{f(x)}$,\n",
    "$$= \\mathbb{E}_{\\mathcal{D},x}[(g(x)-\\bar{f(x)}+ \\bar{f(x)}+\\eta-f(x))^2] = \n",
    "$$\n",
    "\n",
    "Rearranging terms we have,\n",
    "$$= \\mathbb{E}_{\\mathcal{D},x}[((g(x)-\\bar{f(x)})+ (\\bar{f(x)}-f(x)) +\\eta)^2] = \n",
    "$$\n",
    "$$= \\mathbb{E}_{\\mathcal{D},x}[(g(x)-\\bar{f(x)})^2+ (\\bar{f(x)}-f(x))^2 +\\eta^2 + \\text{cross terms}] = \n",
    "$$\n",
    "\n",
    "All cross and single terms are zero (**you can check this at home and deliver the proof**)\n",
    "\n",
    "$$= \\underset{bias^2}{\\underbrace{\\mathbb{E}_{\\mathcal{D},x}[(g(x)-\\bar{f(x)})^2]}}+ \\underset{variance}{\\underbrace{\\mathbb{E}_{\\mathcal{D},x}[(\\bar{f(x)}-f(x))^2]}} + \\underset{stochastic noise}{\\underbrace{\\mathbb{E}_{\\mathcal{D},x}[\\eta^2]}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now plot the learning behavior for a fixed number of examples with respect to the complexity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">**QUESTION: ** What do you expect to happen?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "MAXC=20\n",
    "N=1000\n",
    "NTEST=4000\n",
    "ITERS=3\n",
    "\n",
    "yhat_test=np.zeros((ITERS,MAXC,2))\n",
    "yhat_train=np.zeros((ITERS,MAXC,2))\n",
    "#Repeat ten times to get smooth curves\n",
    "for i in range(ITERS):\n",
    "    X = np.concatenate([1.25*np.random.randn(N,2),5+1.5*np.random.randn(N,2)]) \n",
    "    X = np.concatenate([X,[8,5]+1.5*np.random.randn(N,2)])\n",
    "    y = np.concatenate([np.ones((N,1)),-np.ones((N,1))])\n",
    "    y = np.concatenate([y,np.ones((N,1))])\n",
    "    perm = np.random.permutation(y.size)\n",
    "    X = X[perm,:]\n",
    "    y = y[perm]\n",
    "\n",
    "    X_test = np.concatenate([1.25*np.random.randn(NTEST,2),5+1.5*np.random.randn(NTEST,2)]) \n",
    "    X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(NTEST,2)])\n",
    "    y_test = np.concatenate([np.ones((NTEST,1)),-np.ones((NTEST,1))])\n",
    "    y_test = np.concatenate([y_test,np.ones((NTEST,1))])\n",
    "    \n",
    "    idxplus = y==1\n",
    "    idxminus = y==-1\n",
    "    idxplus = idxplus.flatten()\n",
    "    idxminus = idxminus.flatten()\n",
    "    j=0\n",
    "    for C in range(1,MAXC+1):\n",
    "        #Evaluate the model\n",
    "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
    "        clf.fit(X,y.ravel())\n",
    "        yhat_test[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
    "        yhat_train[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X), y.ravel())\n",
    "        j=j+1\n",
    "\n",
    "p1, = plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),'r')\n",
    "p2, = plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'b')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,5)\n",
    "plt.xlabel('Complexity')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend([p1, p2], [\"Testing error\", \"Training error\"])\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that as the complexity increases the training error is reduced but above a certain complexity level the test error increases. This effect is called **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cures to overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reasoning on how to cure overfitting comes from the very well known bound\n",
    "$$E_{out}\\leq E_{in} + \\mathcal{O}(\\sqrt{\\frac{C}{N}})$$\n",
    "\n",
    "We want to control the out-of-sample error, thus we have a couple of routes to take:\n",
    "\n",
    "1. Either we simulate the out-of-sample error and check against unseen data. This is working directly with the left hand side of the inequality. We can do this using **cross-validation**.\n",
    "2. We change the learning objective to take into account and minimize the complexity of the model. We mimick the right hand side of the inequality and add to the objective function a term penalizing complexity. This is called **regularization**.\n",
    "\n",
    "3. We can use **ensemble techniques**. Implicitly this strategy is similar to the complexity control technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
